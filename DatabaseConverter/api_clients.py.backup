import requests
import time
import os
import threading
from typing import Dict, List, Optional, Any

class ResolverClient:
    """Client for batch resolution using resolver service"""
    
    def __init__(self):
        self.resolver_url = os.getenv("RESOLVER_URL")
        self.resolver_token = os.getenv("RESOLVER_TOKEN")
        
        # Councils that work with the current resolver (IDOX-based)
        self.idox_ok = {
            "Barnet", "Brent", "Enfield", "Haringey", "Westminster", "Camden", "Islington", "Hackney",
            "Harrow", "Ealing", "Hounslow", "KensingtonAndChelsea", "Southwark", "Lambeth", "Lewisham",
            "Croydon", "Merton", "Hammersmith and Fulham", "Hillingdon", "Havering", "Newham",
            "Redbridge", "Greenwich", "Barking and Dagenham", "Bexley", "Kingston upon Thames",
            "Sutton", "Tower Hamlets", "Bromley"  # Added Bromley for outline applications
        }
    
    def resolve_batch_items(self, rows):
        """Resolve a batch of items using the resolver service
        
        Args:
            rows: List of dicts with {"ref": "...", "borough": "...", "app_index": ...} structure
        
        Returns:
            List of resolved URLs in same order as input, or empty list if no supported boroughs
        """
        # Only keep rows where borough is in IDOX_OK
        items = [r for r in rows if r["borough"] in self.idox_ok]
        
        if not items:
            print(f"No supported boroughs found in {len(rows)} items")
            return []  # nothing to resolve
        
        # Group by borough since API expects one borough per request
        borough_groups = {}
        for item in items:
            borough = item["borough"]
            if borough not in borough_groups:
                borough_groups[borough] = []
            borough_groups[borough].append(item)
        
        # Ensure URL has proper scheme
        url = self.resolver_url
        if not url.startswith(('http://', 'https://')):
            url = f"https://{url}"
        
        # Results in same order as input
        all_results = [{"url": "N/A"} for _ in items]
        
        # Process each borough separately  
        for borough, borough_items in borough_groups.items():
            try:
                refs = [item["ref"] for item in borough_items]
                timeout = max(180, 20 + 35 * len(refs))
                
                r = requests.post(
                    f"{url}/resolve-batch",
                    headers={
                        "Authorization": f"Bearer {self.resolver_token}",
                        "Content-Type": "application/json"
                    },
                    json={"borough": borough, "refs": refs},
                    timeout=timeout
                )
                r.raise_for_status()
                data = r.json()
                borough_results = data.get("results", data) if isinstance(data, dict) else data
                if borough_results is None:
                    borough_results = []
                
                print(f"Resolver response type: {type(data).__name__}, len={len(borough_results)}")
                
                # Map results back to original positions
                for i, result in enumerate(borough_results):
                    if i < len(borough_items):
                        # Find the position in the original items list
                        original_item = borough_items[i]
                        original_index = next(j for j, item in enumerate(items) if item is original_item)
                        all_results[original_index] = result
                        
                print(f"Successfully resolved {len(borough_results)} URLs from {len(refs)} items in {borough}")
                
            except Exception as e:
                print(f"‚ùå Failed to resolve {len(refs)} items from {borough}: {e}")
                # Results already initialized as N/A, so no need to change anything
        
        return all_results

class CompaniesHouseClient:
    """Client for interacting with Companies House API with global rate limiting"""
    
    # Global rate limiter shared across all instances and threads
    _last_request_time = 0
    _rate_lock = threading.Lock()
    _min_interval = 0.75  # Conservative: ~80 requests/minute instead of 100
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.company-information.service.gov.uk"
        self.session = requests.Session()
        self.session.auth = (api_key, '')
        self.session.headers.update({
            'User-Agent': 'UK-Company-Enrichment-App/1.0'
        })
    
    def _make_request(self, endpoint: str, params: Optional[Dict] = None, retry_count: int = 0) -> Optional[Dict]:
        """Make a globally rate-limited request to Companies House API"""
        if not self.api_key:
            print("‚ùå Companies House API key not configured")
            return None
        
        max_retries = 2
        
        # Global rate limiting - all threads coordinate here
        with CompaniesHouseClient._rate_lock:
            now = time.time()
            time_since_last = now - CompaniesHouseClient._last_request_time
            
            if time_since_last < CompaniesHouseClient._min_interval:
                sleep_time = CompaniesHouseClient._min_interval - time_since_last
                time.sleep(sleep_time)
            
            CompaniesHouseClient._last_request_time = time.time()
        
        try:
            url = f"{self.base_url}{endpoint}"
            response = self.session.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 401:
                print("‚ùå Invalid Companies House API key")
                return None
            elif response.status_code == 404:
                return None
            elif response.status_code == 429:
                if retry_count < max_retries:
                    # Respect Retry-After header if present, otherwise short delay
                    retry_after = response.headers.get('Retry-After')
                    if retry_after and retry_after.isdigit():
                        delay = min(int(retry_after), 10)  # Cap at 10 seconds
                    else:
                        delay = 3 + (retry_count * 2)  # Progressive: 3s, 5s
                    
                    print(f"‚è≥ Rate limited. Retrying in {delay}s...")
                    time.sleep(delay)
                    return self._make_request(endpoint, params, retry_count + 1)
                else:
                    print(f"‚ùå Rate limit exceeded. Skipping this request.")
                    return None
            else:
                print(f"‚ùå API request failed with status {response.status_code}")
                return None
        
        except requests.RequestException as e:
            if retry_count < max_retries:
                delay = 2 + retry_count
                print(f"‚è≥ Connection error. Retrying in {delay}s: {str(e)[:50]}...")
                time.sleep(delay)
                return self._make_request(endpoint, params, retry_count + 1)
            else:
                print(f"‚ùå Connection failed: {str(e)[:50]}...")
                return None
    
    def search_companies(self, query: str, items_per_page: int = 20) -> List[Dict]:
        """Search for companies by name"""
        endpoint = "/search/companies"
        params = {
            'q': query,
            'items_per_page': items_per_page
        }
        
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def get_company_details(self, company_number: str) -> Optional[Dict]:
        """Get detailed information about a company"""
        endpoint = f"/company/{company_number}"
        return self._make_request(endpoint)
    
    def get_company_officers(self, company_number: str) -> List[Dict]:
        """Get company officers"""
        endpoint = f"/company/{company_number}/officers"
        response = self._make_request(endpoint)
        if response and 'items' in response:
            return response['items']
        return []
    
    def get_company_filing_history(self, company_number: str, items_per_page: int = 20) -> List[Dict]:
        """Get company filing history"""
        endpoint = f"/company/{company_number}/filing-history"
        params = {'items_per_page': items_per_page}
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def get_company_charges(self, company_number: str, items_per_page: int = 25) -> List[Dict]:
        """Get company charges"""
        endpoint = f"/company/{company_number}/charges"
        params = {'items_per_page': items_per_page}
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def search_companies_by_sic(self, sic_code: str, items_per_page: int = 20) -> List[Dict]:
        """Search for companies by SIC code using advanced search"""
        endpoint = "/advanced-search/companies"
        params = {
            'sic_codes': sic_code,
            'size': str(items_per_page)
        }
        
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def search_companies_by_status(self, status: str, items_per_page: int = 20) -> List[Dict]:
        """Search for companies by status using advanced search"""
        endpoint = "/advanced-search/companies"
        params = {
            'company_status': status,
            'size': str(items_per_page)
        }
        
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def search_companies_by_incorporation_date(self, date_from, date_to, location_filter: str = "", items_per_page: int = 20) -> List[Dict]:
        """Search for companies by incorporation date range using advanced search"""
        endpoint = "/advanced-search/companies"
        
        # Format dates for API
        date_from_str = date_from.strftime('%Y-%m-%d')
        date_to_str = date_to.strftime('%Y-%m-%d')
        
        params = {
            'incorporated_from': date_from_str,
            'incorporated_to': date_to_str,
            'size': str(items_per_page)
        }
        
        if location_filter:
            params['location'] = location_filter
        
        response = self._make_request(endpoint, params)
        if response and 'items' in response:
            return response['items']
        return []
    
    def check_health(self) -> Dict[str, Any]:
        """Check Companies House API client health status"""
        health_status = {
            'healthy': False,
            'api_key_configured': bool(self.api_key and self.api_key.strip()),
            'api_accessible': False,
            'error_message': None
        }
        
        try:
            if not self.api_key or not self.api_key.strip():
                health_status['error_message'] = "COMPANIES_HOUSE_API_KEY not configured"
                return health_status
            
            # Test API access with a simple search
            response = self._make_request("/search/companies", {'q': 'test', 'items_per_page': 1})
            if response is not None:
                health_status['api_accessible'] = True
                health_status['healthy'] = True
            else:
                health_status['error_message'] = "API request failed or returned None"
                
        except Exception as e:
            health_status['error_message'] = str(e)
            
        return health_status

    def search_companies_combined(self, sic_code: str, status: str, date_from, location_filter: str = "", max_results: int = 300) -> List[Dict]:
        """Search for companies using combined criteria: SIC code, status, and incorporation date with pagination using advanced search"""
        endpoint = "/advanced-search/companies"
        
        # Build parameters combining criteria
        params = {}
        
        if sic_code and sic_code.strip():
            params['sic_codes'] = sic_code.strip()
        
        if status and status != "all":
            params['company_status'] = status
        
        if date_from:
            date_from_str = date_from.strftime('%Y-%m-%d')
            params['incorporated_from'] = date_from_str
        
        if location_filter:
            params['location'] = location_filter
        
        # No search criteria provided
        if not params:
            return []
        
        all_results = []
        
        # FIXED: Prevent hanging with smaller page sizes and limits
        page_size = min(100, max_results)  # Much smaller page size to prevent hanging
        start_index = 0
        max_iterations = 5  # Limit to 5 API calls maximum to prevent infinite loops
        iteration_count = 0
        
        while len(all_results) < max_results and iteration_count < max_iterations:
            iteration_count += 1
            
            # Calculate how many more items we need
            remaining = max_results - len(all_results)
            current_page_size = min(page_size, remaining)
            
            # Set pagination parameters
            current_params = params.copy()
            current_params['size'] = str(current_page_size)
            current_params['start_index'] = str(start_index)
            
            try:
                response = self._make_request(endpoint, current_params)
                if response and 'items' in response:
                    items = response['items']
                    
                    # If no items returned, we've reached the end
                    if not items:
                        break
                    
                    # Add items to results
                    all_results.extend(items)
                    
                    # If we got fewer items than requested, we've reached the end
                    if len(items) < current_page_size:
                        break
                    
                    # Update start_index for next page
                    start_index += len(items)
                    
                else:
                    # API request failed, break the loop
                    break
                    
            except Exception as e:
                print(f"Error in search iteration {iteration_count}: {e}")
                break
        
        # Trim to exact number requested
        final_results = all_results[:max_results]
        
        return final_results
    
    def get_companies_batch(self, company_numbers: List[str]) -> Dict[str, Optional[Dict]]:
        """Get details for multiple companies in batch with rate limiting"""
        results = {}
        
        for company_number in company_numbers:
            if not company_number:
                results[company_number] = None
                continue
            
            try:
                company_data = self.get_company_details(company_number)
                results[company_number] = company_data
                
                # Log successful fetch
                if company_data:
                    print(f"‚úÖ Fetched company: {company_number}")
                else:
                    print(f"‚ùå Company not found: {company_number}")
                    
            except Exception as e:
                print(f"‚ùå Error fetching company {company_number}: {e}")
                results[company_number] = None
        
        return results
    
    def get_officers_batch(self, company_numbers: List[str]) -> Dict[str, List[Dict]]:
        """Get officers for multiple companies in batch with rate limiting"""
        results = {}
        
        for company_number in company_numbers:
            if not company_number:
                results[company_number] = []
                continue
            
            try:
                officers = self.get_company_officers(company_number)
                results[company_number] = officers
                
                # Log successful fetch
                print(f"‚úÖ Fetched {len(officers)} officers for company: {company_number}")
                    
            except Exception as e:
                print(f"‚ùå Error fetching officers for {company_number}: {e}")
                results[company_number] = []
        
        return results


class ClearbitClient:
    """Client for Clearbit API"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://company.clearbit.com/v2"
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}'
        })
    
    def enrich_company(self, company_data: Dict) -> Optional[Dict]:
        """Enrich company data using Clearbit"""
        if not self.api_key:
            return None
        
        try:
            domain = self._extract_domain_from_company(company_data)
            if not domain:
                return None
            
            endpoint = f"{self.base_url}/companies/find"
            params = {"domain": domain}
            
            response = self.session.get(endpoint, params=params)
            
            if response.status_code == 200:
                data = response.json()
                return self._format_clearbit_response(data)
            
            return None
        
        except Exception as e:
            print(f"‚ö†Ô∏è Clearbit enrichment failed: {str(e)}")
            return None
    
    def _extract_domain_from_company(self, company_data: Dict) -> Optional[str]:
        """Extract or guess domain from company data"""
        company_name = company_data.get('company_name', '').lower()
        if not company_name:
            return None
        
        # Simple domain guessing
        domain_guess = company_name.replace(' ', '').replace('ltd', '').replace('limited', '').replace('plc', '')
        domain_guess = ''.join(c for c in domain_guess if c.isalnum())
        return f"{domain_guess}.com"
    
    def _format_clearbit_response(self, data: Dict) -> Dict:
        """Format Clearbit response to standardized format"""
        return {
            'name': data.get('name'),
            'domain': data.get('domain'),
            'industry': data.get('category', {}).get('industry'),
            'employee_count': data.get('metrics', {}).get('employees'),
            'annual_revenue': data.get('metrics', {}).get('annualRevenue'),
            'description': data.get('description'),
            'founded_year': data.get('foundedYear'),
            'location': {
                'city': data.get('geo', {}).get('city'),
                'state': data.get('geo', {}).get('state'),
                'country': data.get('geo', {}).get('country')
            },
            'technologies': data.get('tech', []),
            'social_profiles': {
                'linkedin': data.get('linkedin', {}).get('handle'),
                'twitter': data.get('twitter', {}).get('handle'),
                'facebook': data.get('facebook', {}).get('handle')
            }
        }




class LondonPlanningClient:
    """London Planning Data API client for planning applications across all 35 London boroughs
    
    Uses the official London planning data hub API with direct Elasticsearch integration
    """
    
    def __init__(self):
        self.base_url = "https://planningdata.london.gov.uk/api-guest/applications/_search"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'UK-Planning-Search/1.0',
            'X-API-AllowRequest': 'be2rmRnt&',
            'Content-Type': 'application/json'
        })
        
        # Cache for Reference ‚Üí URL mapping to avoid repeated requests
        self.keyval_cache = {}
        
        # Available London boroughs
        self.london_boroughs = [
            'Westminster', 'Camden', 'Islington', 'Hackney', 'Tower Hamlets', 
            'Greenwich', 'Lewisham', 'Southwark', 'Lambeth', 'Wandsworth',
            'Hammersmith and Fulham', 'Kensington and Chelsea', 'Brent', 'Ealing',
            'Hounslow', 'Richmond upon Thames', 'Kingston upon Thames', 'Merton',
            'Sutton', 'Croydon', 'Bromley', 'Lewisham', 'Bexley', 'Havering',
            'Barking and Dagenham', 'Redbridge', 'Newham', 'Waltham Forest',
            'Haringey', 'Enfield', 'Barnet', 'Harrow', 'Hillingdon'
        ]
        
        # Web scraping headers for keyVal resolution
        self.scrape_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/126.0.0.0 Safari/537.36"
        }
        
        # Portal mappings for keyVal resolution - all 33 London boroughs
        self.idox_portals = {
            # London Boroughs - Idox Planning Portals (support keyVal resolution)
            'barnet': 'https://publicaccess.barnet.gov.uk/online-applications',
            'westminster': 'https://idoxpa.westminster.gov.uk/online-applications', 
            'camden': 'https://planning.camden.gov.uk/online-applications',
            'hackney': 'https://planning.hackney.gov.uk/online-applications',
            'islington': 'https://planning.islington.gov.uk/online-applications',
            'tower_hamlets': 'https://development.towerhamlets.gov.uk/online-applications',
            'southwark': 'https://planning.southwark.gov.uk/online-applications',
            'lambeth': 'https://planning.lambeth.gov.uk/online-applications',
            'wandsworth': 'https://planning.wandsworth.gov.uk/online-applications',
            'kingston_upon_thames': 'https://planning.kingston.gov.uk/online-applications',
            'merton': 'https://planning.merton.gov.uk/online-applications',
            'sutton': 'https://secplan.sutton.gov.uk/online-applications',
            'croydon': 'https://publicaccess2.croydon.gov.uk/online-applications',
            'bromley': 'https://searchapplications.bromley.gov.uk/online-applications',
            'bexley': 'https://pa.bexley.gov.uk/online-applications',
            'greenwich': 'https://planning.royalgreenwich.gov.uk/online-applications',
            'lewisham': 'https://planning.lewisham.gov.uk/online-applications',
            'newham': 'https://pa.newham.gov.uk/online-applications',
            'waltham_forest': 'https://planning.walthamforest.gov.uk/online-applications',
            'redbridge': 'https://planning.redbridge.gov.uk/online-applications',
            'havering': 'https://pa2.havering.gov.uk/online-applications',
            'enfield': 'https://planningandbuildingcontrol.enfield.gov.uk/online-applications',
            'brent': 'https://pa.brent.gov.uk/online-applications',
            'ealing': 'https://pam.ealing.gov.uk/online-applications',
            'harrow': 'https://planning.harrow.gov.uk/online-applications',
            'hillingdon': 'https://planning.hillingdon.gov.uk/online-applications',
            'haringey': 'https://www.planningservices.haringey.gov.uk/online-applications',
            'hammersmith_and_fulham': 'https://public-access.lbhf.gov.uk/online-applications',
            'barking_and_dagenham': 'https://paplan.lbbd.gov.uk/online-applications',
            'city_of_london': 'https://www.planning2.cityoflondon.gov.uk/online-applications'
        }
        
        print("üíæ Reference ‚Üí URL caching enabled")
    
    def clear_url_cache(self):
        """Clear the cached URLs to force fresh resolution"""
        self.keyval_cache.clear()
        print("üßπ URL cache cleared")
        
        # Web scraping headers for keyVal resolution
        self.scrape_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/126.0.0.0 Safari/537.36"
        }
        
        # Portal mappings for keyVal resolution - all 33 London boroughs
        self.idox_portals = {
            # London Boroughs - Idox Planning Portals (support keyVal resolution)
            'barnet': 'https://publicaccess.barnet.gov.uk/online-applications',
            'westminster': 'https://idoxpa.westminster.gov.uk/online-applications', 
            'camden': 'https://planning.camden.gov.uk/online-applications',
            'hackney': 'https://planning.hackney.gov.uk/online-applications',
            'islington': 'https://planning.islington.gov.uk/online-applications',
            'tower_hamlets': 'https://development.towerhamlets.gov.uk/online-applications',
            'southwark': 'https://planning.southwark.gov.uk/online-applications',
            'lambeth': 'https://planning.lambeth.gov.uk/online-applications',
            'wandsworth': 'https://planning.wandsworth.gov.uk/online-applications',
            'kingston_upon_thames': 'https://planning.kingston.gov.uk/online-applications',
            'merton': 'https://planning.merton.gov.uk/online-applications',
            'sutton': 'https://secplan.sutton.gov.uk/online-applications',
            'croydon': 'https://publicaccess2.croydon.gov.uk/online-applications',
            'bromley': 'https://searchapplications.bromley.gov.uk/online-applications',
            'bexley': 'https://pa.bexley.gov.uk/online-applications',
            'greenwich': 'https://planning.royalgreenwich.gov.uk/online-applications',
            'lewisham': 'https://planning.lewisham.gov.uk/online-applications',
            'newham': 'https://pa.newham.gov.uk/online-applications',
            'waltham_forest': 'https://planning.walthamforest.gov.uk/online-applications',
            'redbridge': 'https://planning.redbridge.gov.uk/online-applications',
            'havering': 'https://pa2.havering.gov.uk/online-applications',
            'enfield': 'https://planningandbuildingcontrol.enfield.gov.uk/online-applications',
            'brent': 'https://pa.brent.gov.uk/online-applications',
            'ealing': 'https://pam.ealing.gov.uk/online-applications',
            'harrow': 'https://planning.harrow.gov.uk/online-applications',
            'hillingdon': 'https://planning.hillingdon.gov.uk/online-applications',
            'haringey': 'https://www.planningservices.haringey.gov.uk/online-applications',
            'hammersmith_and_fulham': 'https://public-access.lbhf.gov.uk/online-applications',
            'barking_and_dagenham': 'https://paplan.lbbd.gov.uk/online-applications',
            'city_of_london': 'https://www.planning2.cityoflondon.gov.uk/online-applications'
        }
        
        # Non-Idox portals - custom URL patterns
        self.custom_portals = {
            'richmond_upon_thames': {
                'base': 'https://www2.richmond.gov.uk/lbrplanning',
                'search_pattern': '/Planning_CaseNo.aspx?strCASENO='
            },
            'hounslow': {
                'base': 'https://planning.hounslow.gov.uk',
                'search_pattern': '/planning_summary.aspx?strCASENO='
            },
            'kensington_and_chelsea': {
                'base': 'https://www.rbkc.gov.uk/planning',
                'search_pattern': '/searches?reference='
            }
        }
        
        # Contact parsing tabs to try for enhanced applicant data
        self.contact_tabs = ["contacts", "people", "neighbourComments"]
    
    def search_planning_applications(self, local_authority: str = None, 
                                   application_type: str = None, 
                                   start_date: str = None, 
                                   limit: int = 50,
                                   decision_status: str = None,
                                   offset: int = 0,
                                   enable_large_search: bool = False,
                                   enable_outline_filter: bool = False) -> List[Dict]:
        """Search London planning applications with filtering
        
        Args:
            local_authority: London borough name (e.g., 'Westminster', 'Camden')
            application_type: Type of planning application 
            start_date: Start date in YYYY-MM-DD format
            limit: Maximum number of results (default: 50)
            decision_status: Decision status filter (e.g., 'Approved', 'Refused', 'Pending')
            offset: Pagination offset (default: 0)
            enable_large_search: Enable smart pagination for >10K results
            enable_outline_filter: Apply server-side outline filtering to eliminate sampling bias
        
        Returns:
            List of planning applications from London API
        """
        print(f"üèôÔ∏è SEARCHING LONDON PLANNING APPLICATIONS...")
        print(f"üèõÔ∏è Authority: {local_authority or 'All London boroughs'}")
        print(f"üìÖ From date: {start_date or 'Any date'}")
        print(f"üìã Type: {application_type or 'All types'}")
        
        # Check if outline filtering is enabled
        if enable_outline_filter:
            print(f"üéØ ‚úÖ Server-side outline filtering ENABLED - eliminating sampling bias")
        
        # Build Elasticsearch query according to API documentation
        query = {"bool": {"must": []}}
        
        # Add local authority filter if specified
        if local_authority and local_authority.strip():
            print(f"üèõÔ∏è ‚úÖ Adding authority filter: {local_authority}")
            # Use exact term matching with .raw field for precise matching
            query["bool"]["must"].append({
                "term": {"lpa_name.raw": local_authority}
            })
        
        # Add date filter if specified
        if start_date and start_date.strip():
            print(f"üìÖ ‚úÖ Adding date filter from: {start_date}")
            # Convert to DD/MM/YYYY format as shown in API documentation
            try:
                from datetime import datetime
                date_obj = datetime.strptime(start_date, '%Y-%m-%d')
                formatted_date = date_obj.strftime('%d/%m/%Y')
                query["bool"]["must"].append({
                    "range": {"valid_date": {"gte": formatted_date}}
                })
            except ValueError:
                print(f"‚ö†Ô∏è Invalid date format: {start_date}, skipping date filter")
        
        # Add application type filter if specified  
        if application_type and application_type.strip():
            print(f"üìã ‚úÖ Adding application type filter: {application_type}")
            # Use exact term matching with .raw field for precise matching
            query["bool"]["must"].append({
                "term": {"application_type.raw": application_type}
            })
        
        # Add server-side outline filter if enabled
        if enable_outline_filter:
            from utils import create_outline_elasticsearch_query
            outline_query = create_outline_elasticsearch_query()
            query["bool"]["must"].append(outline_query)
            print(f"üéØ ‚úÖ Added server-side outline filter to Elasticsearch query")
        
        # Add decision status filter if specified
        if decision_status and decision_status.strip() and decision_status != "All Statuses":
            print(f"‚öñÔ∏è ‚úÖ Adding decision status filter: {decision_status}")
            # Use exact term matching for decision status with .raw field for precise matching
            query["bool"]["must"].append({
                "term": {"decision.raw": decision_status}
            })
        
        # If no filters specified, add a broad match_all query
        if not query["bool"]["must"]:
            query = {"match_all": {}}
            print("üåê No filters specified - searching all London applications")
        
        # Build request body according to API documentation
        request_body = {
            "query": query,
            "size": limit,
            "from": offset,  # Add offset for pagination
            "_source": [
                "lpa_name", "lpa_app_no", "last_updated", "valid_date", 
                "decision_date", "decision", "decision_status", "status", "id", "application_type", "description", 
                "development_description", "proposal_description", "work_description",
                "applicant", "applicant_name", "organisation", "name"  # Add applicant fields
            ]
        }
        
        # Handle large searches by implementing smart pagination strategies
        if enable_large_search and limit > 10000:
            return self._search_large_dataset(query, limit, start_date, local_authority, application_type, decision_status, enable_outline_filter)
        
        try:
            print(f"üîÑ Making API request to London Planning API...")
            print(f"üîç DEBUG: Request body = {request_body}")
            
            # Try the request with retry logic
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.post(
                        self.base_url,
                        json=request_body,
                        timeout=90  # Increased timeout to 90 seconds
                    )
                    break  # Success, exit retry loop
                except Exception as e:
                    print(f"‚ùå Attempt {attempt + 1} failed: {str(e)}")
                    if attempt < max_retries - 1:
                        print(f"‚è≥ Retrying in 2 seconds...")
                        import time
                        time.sleep(2)
                    else:
                        raise  # Re-raise the exception if all retries failed
            
            print(f"üì° API Response Status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                hits = data.get('hits', {}).get('hits', [])
                
                print(f"‚úÖ Successfully retrieved {len(hits)} planning applications")
                
                # Debug: Print first record to see actual structure
                if hits:
                    print(f"üîç DEBUG: First record structure = {hits[0].get('_source', {})}")
                    
                    # Debug: Show unique application types in this result set
                    app_types = set()
                    for hit in hits[:20]:  # Check first 20 records
                        source = hit.get('_source', {})
                        app_type = source.get('application_type')
                        if app_type:
                            app_types.add(app_type)
                    print(f"üîç DEBUG: Application types found = {sorted(list(app_types))}")
                
                # Format results to standardized format
                applications = []
                outline_detected_count = 0
                detection_methods = []
                
                for hit in hits:
                    source = hit.get('_source', {})
                    # Try multiple possible description fields
                    description = (source.get('description') or 
                                 source.get('development_description') or 
                                 source.get('proposal_description') or 
                                 source.get('work_description') or 
                                 'No description available')
                    
                    # Extract applicant information from multiple possible fields
                    applicant = (source.get('applicant') or 
                               source.get('applicant_name') or 
                               source.get('organisation') or 
                               source.get('name') or 
                               'Not specified')
                    
                    app = {
                        'reference': source.get('lpa_app_no'),
                        'authority': source.get('lpa_name'),
                        'application_type': source.get('application_type'),
                        'description': description,
                        'applicant': applicant,  # Add applicant field
                        'valid_date': source.get('valid_date'),
                        'decision_date': source.get('decision_date'),
                        'decision': source.get('decision'),  # Specific decision outcome (Approved, Refused, etc.)
                        'status': source.get('status'),      # Status field
                        'last_updated': source.get('last_updated'),
                        'id': source.get('id')
                    }
                    
                    # If outline filtering was enabled, log detection details
                    if enable_outline_filter:
                        from utils import is_outline
                        if is_outline(app):
                            outline_detected_count += 1
                            # Determine what triggered the detection
                            ref = str(app.get('reference', '')).upper()
                            app_type = str(app.get('application_type', '')).lower()
                            desc = description.lower()
                            
                            method = []
                            if 'outline' in app_type or 'reserved' in app_type:
                                method.append("app_type")
                            if ref.endswith('OUT') or '/OUT' in ref:
                                method.append("reference_pattern")
                            if any(keyword in desc for keyword in ['outline', 'reserved']):
                                method.append("description_keywords")
                                
                            detection_methods.append(f"{app.get('reference', 'N/A')}: {'+'.join(method) if method else 'fallback'}")
                    
                    applications.append(app)
                
                # Enhanced logging for outline searches
                if enable_outline_filter:
                    print(f"üéØ OUTLINE DETECTION SUMMARY:")
                    print(f"   üìä Total applications retrieved: {len(applications)}")
                    print(f"   ‚úÖ Outline applications detected: {outline_detected_count}")
                    if outline_detected_count > 0:
                        print(f"   üîç Detection methods used:")
                        for method_detail in detection_methods[:10]:  # Show first 10
                            print(f"      ‚Ä¢ {method_detail}")
                        if len(detection_methods) > 10:
                            print(f"      ‚Ä¢ ... and {len(detection_methods) - 10} more")
                    else:
                        print(f"   ‚ö†Ô∏è  NO OUTLINE APPLICATIONS found in this search")
                        print(f"   üí° Consider:")
                        print(f"      ‚Ä¢ Expanding date range (try 2020-2025)")
                        print(f"      ‚Ä¢ Searching different boroughs")
                        print(f"      ‚Ä¢ Checking if outline applications use different terminology")
                
                # üîó RESOLVER INTEGRATION: Add planning URLs for outline applications
                if enable_outline_filter and applications:
                    print(f"üîó Resolving planning portal URLs for {len(applications)} outline applications...")
                    
                    # Initialize resolver
                    try:
                        resolver = ResolverClient()
                        
                        # Group applications by borough for separate resolver calls
                        # (resolver fails with mixed-borough batches)
                        from collections import defaultdict
                        borough_groups = defaultdict(list)
                        app_to_index = {}
                        
                        for i, app in enumerate(applications):
                            if app.get('reference') and app.get('authority'):
                                borough = app['authority']
                                borough_groups[borough].append({
                                    'ref': app['reference'], 
                                    'borough': borough,
                                    'app_index': i
                                })
                                app_to_index[f"{borough}_{app['reference']}"] = i
                        
                        if borough_groups:
                            print(f"üìã Grouped {len(applications)} applications into {len(borough_groups)} borough batches")
                            
                            # Initialize all applications with N/A first
                            for app in applications:
                                app['planning_url'] = 'N/A'
                            
                            # Process each borough separately
                            total_resolved = 0
                            for borough, items in borough_groups.items():
                                if len(items) > 0:
                                    print(f"üèõÔ∏è Resolving {len(items)} applications from {borough}...")
                                    
                                    # Call resolver service for this borough only
                                    borough_resolved_urls = resolver.resolve_batch_items(items)
                                    
                                    if borough_resolved_urls:
                                        # Process results for this borough
                                        for j, result in enumerate(borough_resolved_urls):
                                            if j < len(items) and result:
                                                item = items[j]
                                                app_index = item['app_index']
                                                planning_url = result.get('url')
                                                
                                                if planning_url and planning_url != 'N/A' and planning_url is not None:
                                                    # Fix HTML encoding issue: &amp; -> &
                                                    planning_url = planning_url.replace('&amp;', '&')
                                                    applications[app_index]['planning_url'] = planning_url
                                                    total_resolved += 1
                                                    print(f"  ‚úÖ {item['ref']}: {planning_url}")
                                                else:
                                                    print(f"  ‚ùå {item['ref']}: N/A (resolver returned null)")
                                    else:
                                        print(f"  ‚ö†Ô∏è No resolver results for {borough}")
                            
                            print(f"üéØ RESOLVER SUMMARY: {total_resolved} URLs resolved across all boroughs")
                        else:
                            print(f"‚ö†Ô∏è  No valid items to resolve")
                            for app in applications:
                                app['planning_url'] = 'N/A'
                                
                    except Exception as e:
                        # Mask the URL to avoid exposing secret endpoints
                        error_msg = str(e)
                        if "https://" in error_msg and "ngrok" in error_msg:
                            print(f"‚ùå Resolver service connection error (endpoint unavailable)")
                        else:
                            print(f"‚ùå Resolver error: {error_msg}")
                        # Set all URLs to N/A on resolver failure
                        for app in applications:
                            app['planning_url'] = 'N/A'
                
                return applications
                
            else:
                print(f"‚ùå API Error {response.status_code}: {response.text[:200]}")
                return []
                
        except Exception as e:
            print(f"‚ùå Exception occurred: {str(e)}")
            return []
    
    def _search_large_dataset(self, query, target_limit, start_date=None, local_authority=None, application_type=None, decision_status=None, enable_outline_filter=False):
        """Handle large dataset searches using multiple pagination strategies"""
        print(f"üöÄ LARGE SEARCH MODE: Target {target_limit:,} results")
        print(f"üìä Using smart pagination to bypass 10K API limit")
        
        all_results = []
        seen_ids = set()
        
        # Strategy 1: Authority-based parallel searches (if no specific authority)
        if local_authority is None or local_authority == "All London Boroughs":
            print(f"üìç STRATEGY 1: Authority-based parallel searches")
            authorities_to_search = self.london_boroughs[:10]  # Start with top 10 boroughs
            
            for authority in authorities_to_search:
                print(f"   Searching {authority}...")
                authority_results = self.search_planning_applications(
                    local_authority=authority,
                    application_type=application_type,
                    start_date=start_date,
                    limit=10000,  # Max per authority
                    decision_status=decision_status,
                    enable_large_search=False,  # Avoid recursion
                    enable_outline_filter=enable_outline_filter
                )
                
                # Add unique results
                for result in authority_results:
                    result_id = result.get('id')
                    if result_id and result_id not in seen_ids:
                        seen_ids.add(result_id)
                        all_results.append(result)
                        
                        # Stop if we reach target
                        if len(all_results) >= target_limit:
                            print(f"üéØ Reached target: {len(all_results):,} results")
                            return all_results[:target_limit]
                
                print(f"   {authority}: {len(authority_results)} results (total: {len(all_results)})")
        
        # Strategy 2: Date-based chunking (if we still need more results)
        if len(all_results) < target_limit and start_date:
            print(f"üìÖ STRATEGY 2: Date-based chunking (need {target_limit - len(all_results):,} more)")
            
            from datetime import datetime, timedelta
            import calendar
            
            try:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                current_dt = start_dt
                end_dt = datetime.now()
                
                while current_dt < end_dt and len(all_results) < target_limit:
                    # Get month chunk
                    month_start = current_dt.strftime('%Y-%m-%d')
                    last_day = calendar.monthrange(current_dt.year, current_dt.month)[1]
                    month_end = current_dt.replace(day=last_day)
                    
                    print(f"   Searching month: {current_dt.strftime('%Y-%m')}")
                    
                    # Use offset pagination within this month
                    offset = 0
                    while offset < 50000 and len(all_results) < target_limit:  # Max 5 pages per month
                        chunk_results = self.search_planning_applications(
                            local_authority=local_authority,
                            application_type=application_type,
                            start_date=month_start,
                            limit=min(10000, target_limit - len(all_results)),
                            decision_status=decision_status,
                            offset=offset,
                            enable_large_search=False,  # Avoid recursion
                            enable_outline_filter=enable_outline_filter
                        )
                        
                        if not chunk_results:
                            break  # No more results for this month
                        
                        # Add unique results
                        new_results = 0
                        for result in chunk_results:
                            result_id = result.get('id')
                            if result_id and result_id not in seen_ids:
                                seen_ids.add(result_id)
                                all_results.append(result)
                                new_results += 1
                        
                        print(f"     Offset {offset}: {new_results} new results (total: {len(all_results)})")
                        
                        if new_results == 0:
                            break  # No new results, move to next month
                        
                        offset += 10000
                    
                    # Move to next month
                    if current_dt.month == 12:
                        current_dt = current_dt.replace(year=current_dt.year + 1, month=1, day=1)
                    else:
                        current_dt = current_dt.replace(month=current_dt.month + 1, day=1)
                        
            except ValueError:
                print(f"‚ö†Ô∏è Date chunking failed, using simple offset pagination")
        
        # Strategy 3: Simple offset pagination (fallback)
        if len(all_results) < target_limit:
            print(f"üî¢ STRATEGY 3: Simple offset pagination (need {target_limit - len(all_results):,} more)")
            
            offset = 0
            max_iterations = 20  # Prevent infinite loops
            
            for i in range(max_iterations):
                if len(all_results) >= target_limit:
                    break
                    
                print(f"   Offset {offset}: requesting batch...")
                batch_results = self.search_planning_applications(
                    local_authority=local_authority,
                    application_type=application_type,
                    start_date=start_date,
                    limit=10000,
                    decision_status=decision_status,
                    offset=offset,
                    enable_large_search=False,  # Avoid recursion
                    enable_outline_filter=enable_outline_filter
                )
                
                if not batch_results:
                    print(f"   No more results at offset {offset}")
                    break
                
                # Add unique results
                new_results = 0
                for result in batch_results:
                    result_id = result.get('id')
                    if result_id and result_id not in seen_ids:
                        seen_ids.add(result_id)
                        all_results.append(result)
                        new_results += 1
                
                print(f"   Added {new_results} new results (total: {len(all_results)})")
                
                if new_results == 0:
                    print(f"   No new unique results, stopping pagination")
                    break
                
                offset += 10000
        
        print(f"üéâ LARGE SEARCH COMPLETE: {len(all_results):,} total results")
        return all_results[:target_limit]

    def get_london_boroughs(self) -> List[str]:
        """Get list of available London boroughs"""
        return self.london_boroughs.copy()
    
    # ========== KEYVAL RESOLUTION SYSTEM (ADAPTED FROM BARNET SCRIPT) ==========
    
    def _normalise_whitespace(self, text: str) -> str:
        """Normalize whitespace in text"""
        import re
        return re.sub(r"\s+", " ", text).strip()
    
    def _absolutise_url(self, base: str, href: str) -> str:
        """Convert relative URL to absolute URL"""
        if href.startswith("http"):
            return href
        if not href.startswith("/"):
            href = "/" + href
        return base.rstrip("/") + href
    
    def _pick_first_app_details_link(self, html: str) -> Optional[str]:
        """Find first applicationDetails.do link in HTML"""
        try:
            from bs4 import BeautifulSoup
            import re
            soup = BeautifulSoup(html, "html.parser")
            a = soup.find("a", href=re.compile(r"applicationDetails\.do"))
            if a and a.get("href"):
                return a["href"]
        except:
            pass
        return None
    
    def _try_direct_reference(self, ref: str, base_url: str) -> Optional[str]:
        """Try direct reference URL with Bright Data proxy"""
        import requests
        import re
        url = f"{base_url}/applicationDetails.do?reference={ref}"
        try:
            # Direct request to planning portal
            session = requests.Session()
            session.headers.update(self.scrape_headers)
            print(f"üîç Direct request: {url}")
            
            r = session.get(url, allow_redirects=True, timeout=15)
            if r.status_code == 200 and "applicationDetails" in r.url:
                return r.url
            # Fallback: simple content check for ref text
            if r.status_code == 200 and ref.replace(" ", "").lower() in re.sub(r"\s+", "", r.text).lower():
                return r.url
        except Exception as e:
            # Better error logging
            print(f"‚ùå Direct reference method failed for {ref}: {str(e)}")
            pass
        return None
    
    def _try_search_get(self, ref: str, base_url: str) -> Optional[str]:
        """Try GET search method with Bright Data proxy"""
        import requests
        search_url = f"{base_url}/search.do?action=search&searchType=Application&reference={ref}"
        try:
            # Direct search request to planning portal
            session = requests.Session()
            session.headers.update(self.scrape_headers)
            print(f"üîç Direct search GET: {search_url}")
            
            r = session.get(search_url, allow_redirects=True, timeout=15)
            if r.status_code != 200:
                print(f"‚ùå Search GET failed for {ref}: HTTP {r.status_code}")
                return None
            link = self._pick_first_app_details_link(r.text)
            if link:
                resolved_url = self._absolutise_url(base_url, link)
                print(f"‚úÖ Search GET resolved {ref} ‚Üí {resolved_url}")
                return resolved_url
        except Exception as e:
            print(f"‚ùå Search GET method failed for {ref}: {str(e)}")
            pass
        return None
    
    def _try_search_post(self, ref: str, base_url: str) -> Optional[str]:
        """Try POST search method with Bright Data proxy"""
        import requests
        try:
            # Initialize session with proper headers
            session = requests.Session()
            session.headers.update(self.scrape_headers)
            print(f"üîç Direct search POST: {ref}")
            
            # Get advanced search page first to establish session and grab any CSRF tokens
            adv_url = f"{base_url}/search.do?action=advanced"
            init_response = session.get(adv_url, timeout=15)
            if init_response.status_code != 200:
                print(f"‚ùå POST search init failed for {ref}: HTTP {init_response.status_code}")
                return None
            
            data = {
                "searchType": "Application",
                "searchCriteria.reference": ref,
                "date(applicationValidatedStart)": "",
                "date(applicationValidatedEnd)": "",
                "caseAddressType": "Application",
            }
            
            r = session.post(f"{base_url}/doSearch.do", data=data, allow_redirects=True, timeout=5)
            if r.status_code != 200:
                print(f"‚ùå POST search failed for {ref}: HTTP {r.status_code}")
                return None
            
            link = self._pick_first_app_details_link(r.text)
            if link:
                resolved_url = self._absolutise_url(base_url, link)
                print(f"‚úÖ Search POST resolved {ref} ‚Üí {resolved_url}")
                return resolved_url
        except Exception as e:
            print(f"‚ùå Search POST method failed for {ref}: {str(e)}")
            pass
        return None
    
    def _extract_keyval_from_url(self, url: str) -> Optional[str]:
        """Extract keyVal parameter from URL"""
        import re
        m = re.search(r"[?&]keyVal=([A-Za-z0-9]+)", url)
        return m.group(1) if m else None
    
    def _ensure_summary_url(self, url: str) -> str:
        """Force activeTab=summary for stability"""
        import re
        if "activeTab=" in url:
            url = re.sub(r"activeTab=[^&]+", "activeTab=summary", url)
        elif "?" in url:
            url = url + "&activeTab=summary"
        else:
            url = url + "?activeTab=summary"
        return url
    
    def _normalize_authority_name(self, authority: str) -> str:
        """Normalize authority name for portal mapping"""
        if not authority:
            return ""
        
        normalized = authority.lower().replace(' ', '_').replace('-', '_')
        
        # Handle special cases
        authority_mappings = {
            'tower_hamlets': 'tower_hamlets',
            'kingston_upon_thames': 'kingston_upon_thames',
            'richmond_upon_thames': 'richmond_upon_thames',
            'hammersmith_and_fulham': 'hammersmith_and_fulham',
            'kensington_and_chelsea': 'kensington_and_chelsea',
            'barking_and_dagenham': 'barking_and_dagenham',
            'waltham_forest': 'waltham_forest',
            'city_of_london': 'city_of_london'
        }
        
        return authority_mappings.get(normalized, normalized)
    
    # ========== CONTACT SCRAPING SYSTEM (ADAPTED FROM BARNET SCRIPT) ==========
    
    def _build_contact_urls(self, application_url: str) -> List[str]:
        """Build candidate URLs for contact/people pages"""
        candidates = []
        import re
        
        if "activeTab=" in application_url:
            for tab in self.contact_tabs:
                candidates.append(re.sub(r"activeTab=[^&]+", f"activeTab={tab}", application_url))
        else:
            sep = "&" if "?" in application_url else "?"
            for tab in self.contact_tabs:
                candidates.append(f"{application_url}{sep}activeTab={tab}")
        
        return list(dict.fromkeys(candidates))  # Remove duplicates while preserving order
    
    def _fetch_html(self, url: str, timeout: int = 15) -> Optional[str]:
        """Fetch HTML content from URL using proxy system"""
        try:
            # Use proxy-enabled request method
            response = self._make_request_with_proxy(url, method='GET', max_retries=3)
            if response and response.status_code == 200 and response.text:
                return response.text
        except Exception as e:
            print(f"‚ùå Failed to fetch HTML from {url}: {str(e)}")
        return None
    
    def _parse_contacts_html(self, html: str) -> Dict[str, str]:
        """Parse contact information from planning application HTML"""
        try:
            from bs4 import BeautifulSoup
            import re
            
            soup = BeautifulSoup(html, "html.parser")
            data = {}
            
            # Look for section headers that indicate applicant/agent sections
            section_labels = {"applicant": ["applicant"], "agent": ["agent"]}
            
            for header_tag in soup.select("h1,h2,h3,h4,strong"):
                label = self._normalise_whitespace(header_tag.get_text(" ")).lower()
                
                for key, needles in section_labels.items():
                    if any(n in label for n in needles):
                        container = None
                        
                        # Find the content container after this header
                        for sib in header_tag.find_all_next():
                            if sib.name in ["h1", "h2", "h3", "h4", "strong"]:
                                break
                            if sib.name in ["ul", "ol", "dl", "table", "div"] and sib.get_text(strip=True):
                                container = sib
                                break
                        
                        if container:
                            text = self._normalise_whitespace(container.get_text(" "))
                            data[f"{key}_block"] = text
            
            # Fallback: look for contact cards
            if not data:
                cards = soup.select(".contact, .contactDetails, .simpleList")
                for c in cards:
                    t = self._normalise_whitespace(c.get_text(" "))
                    if "applicant" in t.lower():
                        data["applicant_block"] = t
                    if "agent" in t.lower():
                        data["agent_block"] = t
            
            # Extract structured fields from blocks
            result = {}
            if "applicant_block" in data:
                applicant_fields = self._extract_contact_fields(data["applicant_block"])
                result.update({f"applicant_{k}": v for k, v in applicant_fields.items()})
            
            if "agent_block" in data:
                agent_fields = self._extract_contact_fields(data["agent_block"])
                result.update({f"agent_{k}": v for k, v in agent_fields.items()})
            
            return result
            
        except Exception as e:
            return {}
    
    def _extract_contact_fields(self, block: str) -> Dict[str, str]:
        """Extract structured contact fields from text block"""
        import re
        
        out = {}
        
        # Extract key-value pairs using regex
        pairs = re.findall(r"([A-Za-z ]{3,30}):\s*([^:]+?)(?=(?:[A-Za-z ]{3,30}:)|$)", block)
        for k, v in pairs:
            key = self._normalise_whitespace(k).lower().replace(" ", "_")
            out[key] = self._normalise_whitespace(v)
        
        # Extract specific fields if not found in key-value pairs
        if "name" not in out:
            m = re.search(r"(?:name|contact)\s*:\s*([^:]+)", block, flags=re.I)
            if m:
                out["name"] = self._normalise_whitespace(m.group(1))
        
        if "company" not in out:
            m = re.search(r"(?:company|organisation)\s*:\s*([^:]+)", block, flags=re.I)
            if m:
                out["company"] = self._normalise_whitespace(m.group(1))
        
        if "address" not in out:
            m = re.search(r"(address)\s*:\s*([^:]+)", block, flags=re.I)
            if m:
                out["address"] = self._normalise_whitespace(m.group(2))
        
        if "telephone" not in out:
            m = re.search(r"(?:tel(?:ephone)?|phone)\s*:\s*([\d +()-]{7,})", block, flags=re.I)
            if m:
                out["telephone"] = self._normalise_whitespace(m.group(1))
        
        if "email" not in out:
            m = re.search(r"([A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,})", block, flags=re.I)
            if m:
                out["email"] = self._normalise_whitespace(m.group(1))
        
        return out
    
    def scrape_enhanced_applicant_data(self, authority: str, reference: str) -> Dict[str, str]:
        """Scrape enhanced applicant data from planning portal"""
        if not authority or not reference:
            return {}
        
        try:
            # First resolve the planning application URL
            url_result = self.resolve_keyval_planning_url(authority, reference, delay=0.3)
            
            if url_result['status'] != 'resolved':
                # Can't scrape if we don't have a direct application URL
                return {}
            
            application_url = url_result['url']
            
            # Try different contact page URLs
            contact_urls = self._build_contact_urls(application_url)
            
            for candidate_url in contact_urls:
                html = self._fetch_html(candidate_url)
                if html:
                    parsed_contacts = self._parse_contacts_html(html)
                    if parsed_contacts:
                        # Successfully found contact information
                        return {
                            'source_url': candidate_url,
                            'scrape_status': 'success',
                            **parsed_contacts
                        }
            
            # No contact info found
            return {'scrape_status': 'no_contacts_found'}
            
        except Exception as e:
            return {'scrape_status': 'error', 'error': str(e)}
    
    def resolve_keyval_planning_url(self, authority: str, reference: str, delay: float = 2.0) -> Dict[str, str]:
        """Resolve keyVal-based planning application URL with caching and slow requests
        
        Returns:
            Dict with 'url', 'status', and 'method' keys
        """
        if not reference or reference == 'N/A' or not authority:
            return {'url': 'N/A', 'status': 'invalid_input', 'method': 'none'}
        
        # Check cache first - avoid repeated requests
        cache_key = f"{authority}_{reference}"
        if cache_key in self.keyval_cache:
            print(f"üíæ Using cached result for {reference}")
            return self.keyval_cache[cache_key]
        
        # Normalize authority name
        normalized_authority = self._normalize_authority_name(authority)
        
        # Check if this authority has an Idox portal (supports keyVal resolution)
        if normalized_authority in self.idox_portals:
            base_url = self.idox_portals[normalized_authority]
            result = self._resolve_idox_portal(reference, base_url, delay)
            # Cache the result
            self.keyval_cache[cache_key] = result
            return result
        
        # Check if this authority has a custom portal
        elif normalized_authority in self.custom_portals:
            custom_info = self.custom_portals[normalized_authority]
            url = custom_info['base'] + custom_info['search_pattern'] + reference
            return {'url': url, 'status': 'custom_portal', 'method': 'direct_url'}
        
        # No fallback for unknown authorities - only keyVal links
        else:
            return {'url': 'N/A', 'status': 'keyval_failed', 'method': 'none'}
    
    def _resolve_idox_portal(self, reference: str, base_url: str, delay: float = 0.5) -> Dict[str, str]:
        """Resolve keyVal for Idox-based portals using multiple strategies"""
        import time
        
        try:
            # Strategy A: Try direct reference
            url = self._try_direct_reference(reference, base_url)
            if url:
                return {'url': self._ensure_summary_url(url), 'status': 'resolved', 'method': 'direct_reference'}
            
            time.sleep(delay)
            
            # Strategy B: Try GET search
            url = self._try_search_get(reference, base_url)
            if url:
                return {'url': self._ensure_summary_url(url), 'status': 'resolved', 'method': 'search_get'}
            
            time.sleep(delay)
            
            # Strategy C: Try POST search
            url = self._try_search_post(reference, base_url)
            if url:
                return {'url': self._ensure_summary_url(url), 'status': 'resolved', 'method': 'search_post'}
            
            # No fallback - return failure if keyVal extraction fails
            return {'url': 'N/A', 'status': 'keyval_failed', 'method': 'none'}
            
        except Exception as e:
            # No fallback - return failure on any error
            return {'url': 'N/A', 'status': 'keyval_failed', 'method': 'none', 'error': str(e)}
    
    def _resolve_keyval_planning_url_old(self, reference: str, organisation_entity: str = None) -> str:
        """Old method - kept for backward compatibility"""
        if not reference or reference == 'N/A':
            return 'N/A'
        
        import requests
        from bs4 import BeautifulSoup
        import re
        
        # Authority-specific planning portal mappings
        # Idox-based portals support keyVal extraction, others get custom fallbacks
        idox_portals = {
            # London Boroughs - Idox Planning Portals
            'barnet': 'https://publicaccess.barnet.gov.uk/online-applications/',
            'westminster': 'https://idoxpa.westminster.gov.uk/online-applications/',
            'camden': 'https://planning.camden.gov.uk/online-applications/',
            'hackney': 'https://planning.hackney.gov.uk/online-applications/',
            'islington': 'https://planning.islington.gov.uk/online-applications/',
            'tower_hamlets': 'https://development.towerhamlets.gov.uk/online-applications/',
            'southwark': 'https://planning.southwark.gov.uk/online-applications/',
            'lambeth': 'https://planning.lambeth.gov.uk/online-applications/',
            'wandsworth': 'https://planning.wandsworth.gov.uk/online-applications/',
            'kingston': 'https://planning.kingston.gov.uk/online-applications/',
            'merton': 'https://planning.merton.gov.uk/online-applications/',
            'sutton': 'https://planning.sutton.gov.uk/online-applications/',
            'croydon': 'https://publicaccess2.croydon.gov.uk/online-applications/',
            'bromley': 'https://searchapplications.bromley.gov.uk/online-applications/',
            'bexley': 'https://pa.bexley.gov.uk/online-applications/',
            'greenwich': 'https://planning.royalgreenwich.gov.uk/online-applications/',
            'lewisham': 'https://planning.lewisham.gov.uk/online-applications/',
            'newham': 'https://pa.newham.gov.uk/online-applications/',
            'waltham_forest': 'https://planning.walthamforest.gov.uk/online-applications/',
            'redbridge': 'https://planning.redbridge.gov.uk/online-applications/',
            'havering': 'https://planning.havering.gov.uk/online-applications/',
            'enfield': 'https://planningandbuildingcontrol.enfield.gov.uk/online-applications/',
            'brent': 'https://pa.brent.gov.uk/online-applications/',
            'ealing': 'https://pam.ealing.gov.uk/online-applications/',
            
            # Major Cities - Idox Portals
            'birmingham': 'https://eplanning.birmingham.gov.uk/Northgate/DocumentApplication/',
            'manchester': 'https://pa.manchester.gov.uk/online-applications/',
            'leeds': 'https://publicaccess.leeds.gov.uk/online-applications/',
            'sheffield': 'https://planning.sheffield.gov.uk/online-applications/',
            'bristol': 'https://planningonline.bristol.gov.uk/online-applications/',
            'newcastle': 'https://publicaccess.newcastle.gov.uk/online-applications/',
            'nottingham': 'https://planningonline.nottinghamcity.gov.uk/online-applications/',
            'leicester': 'https://planning.leicester.gov.uk/online-applications/',
            'bradford': 'https://planning.bradford.gov.uk/online-applications/',
            
            # Other Councils - Idox Portals
            'cherwell': 'https://planningregister.cherwell.gov.uk/Planning/',
            'oxford': 'https://planningregister.oxford.gov.uk/Planning/',
            'west_oxfordshire': 'https://planning.westoxon.gov.uk/Planning/',
        }
        
        # Non-Idox portal fallbacks - these use custom search URLs when keyVal fails
        non_idox_fallbacks = {
            # London Boroughs with custom systems
            'haringey': 'https://www.haringey.gov.uk/planning-and-building-control/planning/planning-applications/search-planning-applications?reference={reference}',
            'richmond': 'https://www2.richmond.gov.uk/lbrplanning/Planning_CaseNo.aspx?strCASENO={reference}',
            'hounslow': 'https://planning.hounslow.gov.uk/planning_summary.aspx?strCASENO={reference}',
            'hillingdon': 'https://planning.hillingdon.gov.uk/OAS/enquiry/search?number={reference}',
            'harrow': 'https://www.harrow.gov.uk/planning-applications/search?reference={reference}',
            
            # Major Cities with custom systems  
            'liverpool': 'https://liverpool.gov.uk/planning-and-building-control/applications/search?reference={reference}',
            'coventry': 'https://planning.coventry.gov.uk/CherwellDC/search.aspx?reference={reference}',
            
            # Councils with custom systems
            'south_oxfordshire': 'https://data.southoxon.gov.uk/ccm/support/search?reference={reference}',
            'vale_of_white_horse': 'https://data.whitehorsedc.gov.uk/java/support/search?reference={reference}',
        }
        
        # Helper function to extract keyVal from search results
        def extract_keyval_from_search(portal_base: str, reference: str) -> Optional[str]:
            try:
                # Build search URL 
                search_url = f"{portal_base}search.do?action=search&searchType=Application&reference={reference}"
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                # Perform search request using proxy system
                response = self._make_request_with_proxy(search_url, method='GET', max_retries=2)
                if not response or response.status_code != 200:
                    return None
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Look for keyVal in links - multiple patterns
                keyval_patterns = [
                    r'keyVal=([A-Z0-9]{10,})',  # Standard keyVal pattern
                    r'keyval=([A-Z0-9]{10,})',  # lowercase variant
                    r'applicationDetails\.do\?.*keyVal=([A-Z0-9]{10,})'  # Full URL pattern
                ]
                
                # Search in all links and href attributes
                for link in soup.find_all(['a', 'link'], href=True):
                    href = link.get('href', '')
                    if href:
                        for pattern in keyval_patterns:
                            match = re.search(pattern, href, re.IGNORECASE)
                            if match:
                                keyval = match.group(1)
                                print(f"üîç Found keyVal {keyval} for {reference}")
                                return keyval
                
                # Also search in JavaScript/text content
                for script in soup.find_all('script'):
                    if hasattr(script, 'string') and script.string:
                        for pattern in keyval_patterns:
                            match = re.search(pattern, script.string, re.IGNORECASE)
                            if match:
                                keyval = match.group(1)
                                print(f"üîç Found keyVal {keyval} in script for {reference}")
                                return keyval
                
                print(f"‚ö†Ô∏è No keyVal found in search results for {reference}")
                return None
                
            except Exception as e:
                print(f"‚ùå Error searching for keyVal for {reference}: {str(e)}")
                return None
        
        # Try to identify authority from organisation_entity or reference
        authority_key = None
        if organisation_entity:
            org_lower = str(organisation_entity).lower()
            # Direct match first - check both Idox and non-Idox
            if org_lower in idox_portals:
                authority_key = org_lower
            elif org_lower in non_idox_fallbacks:
                authority_key = org_lower
            else:
                # Partial match
                for auth_key in list(idox_portals.keys()) + list(non_idox_fallbacks.keys()):
                    if auth_key in org_lower or org_lower in auth_key:
                        authority_key = auth_key
                        break
        
        # If no authority identified, try common London authorities for London references
        if not authority_key and reference:
            # Barnet pattern: 24/1234/FUL or B/1234/24
            if re.match(r'(24|23|22|21|20)/\d+/', reference) or re.match(r'B/\d+/', reference):
                authority_key = 'barnet'
            elif 'WM' in reference.upper():
                authority_key = 'westminster'
            elif 'CAM' in reference.upper():
                authority_key = 'camden'
        
        # Try keyVal resolution for identified authority (Idox portals only)
        if authority_key and authority_key in idox_portals:
            portal_base = idox_portals[authority_key]
            keyval = extract_keyval_from_search(portal_base, reference)
            
            if keyval:
                # Build the working URL with keyVal
                working_url = f"{portal_base}applicationDetails.do?activeTab=summary&keyVal={keyval}"
                print(f"‚úÖ Generated working keyVal URL for {reference}: {working_url}")
                return working_url
            else:
                # Fallback to search URL for Idox portals
                search_url = f"{portal_base}search.do?action=search&searchType=Application&reference={reference}"
                print(f"‚ö†Ô∏è Fallback to search URL for {reference}: {search_url}")
                return search_url
        
        # Handle non-Idox portals with custom URLs
        if authority_key and authority_key in non_idox_fallbacks:
            fallback_template = non_idox_fallbacks[authority_key]
            custom_url = fallback_template.format(reference=reference)
            print(f"üîó Using custom non-Idox URL for {authority_key}: {custom_url}")
            return custom_url
        
        # Try multiple common Idox authorities if no specific match
        common_authorities = ['barnet', 'westminster', 'camden', 'hackney', 'islington']
        for auth in common_authorities:
            if auth in idox_portals:
                portal_base = idox_portals[auth]
                try:
                    keyval = extract_keyval_from_search(portal_base, reference)
                    if keyval:
                        working_url = f"{portal_base}applicationDetails.do?activeTab=summary&keyVal={keyval}"
                        print(f"‚úÖ Found working keyVal URL via {auth} for {reference}: {working_url}")
                        return working_url
                except Exception as e:
                    print(f"‚ö†Ô∏è Authority {auth} failed for {reference}: {str(e)}")
                    continue
        
        # Final fallback - UK Government Planning Portal
        search_ref = reference.replace('/', '%2F')
        fallback_url = f"https://www.gov.uk/search-planning-applications?reference={search_ref}"
        print(f"üèõÔ∏è Final fallback to government portal for {reference}: {fallback_url}")
        return fallback_url

    def _build_planning_application_url(self, reference: str, organisation_entity: str = None) -> str:
        """Build URL link to actual planning application page using keyVal resolution"""
        # Use the actual method with correct parameters
        result = self.resolve_keyval_planning_url(authority=organisation_entity or "barnet", reference=reference)
        return result.get('url', 'N/A')


# Removed broken duplicate HunterClient class - using complete version below
                
                # Smart stopping logic
                if batch_in_date_range == 0:
                    consecutive_empty_batches += 1
                    print(f"‚ö†Ô∏è No applications from {start_date} in this batch (consecutive empty: {consecutive_empty_batches})")
                else:
                    consecutive_empty_batches = 0  # Reset counter
                    print(f"‚úÖ Found {batch_in_date_range} applications from {start_date} onwards in this batch")
                
                # Stop if we've had too many consecutive empty batches
                if consecutive_empty_batches >= max_empty_batches:
                    print(f"üõë Stopping: {max_empty_batches} consecutive batches with no applications from {start_date}")
                    print(f"üìä Total applications in date range: {applications_in_date_range:,}")
                    break
                
                # Store partial results in real-time for immediate display
                if hasattr(self, '_update_live_results'):
                    # Pass total_available (or total_processed) as the total for percentage calculation
                    self._update_live_results(all_outline_applications, total_processed, total_available, requests_made)
                
                # Check if we should continue - stop when we've processed all available or no more results
                if len(entities) < batch_size:
                    print("‚úÖ Reached end of available applications (final batch)")
                    break
                
                # Stop if we've reached our processing limit
                if max_requests and requests_made >= max_requests:
                    print(f"‚úÖ Reached processing limit ({max_requests} requests = {max_requests * batch_size:,} applications)")
                    break
                
                if total_available and total_processed >= total_available:
                    print("‚úÖ Processed target number of applications")
                    break
                
                # Move to next batch
                offset += batch_size
                
                # Small delay to be respectful to the API
                import time
                time.sleep(0.5)
            
            print(f"\nüéâ SMART OUTLINE RETRIEVAL COMPLETE!")
            print(f"üìä Final Results:")
            print(f"   ‚Ä¢ Total outline applications found: {len(all_outline_applications)}")
            print(f"   ‚Ä¢ Applications in date range ({start_date}+): {applications_in_date_range:,}")
            print(f"   ‚Ä¢ Total applications processed: {total_processed:,}")
            print(f"   ‚Ä¢ API requests made: {requests_made}")
            if applications_in_date_range > 0:
                print(f"   ‚Ä¢ Outline percentage in date range: {len(all_outline_applications)/applications_in_date_range*100:.2f}%")
            
            results = {
                'total_outline_applications': len(all_outline_applications),
                'outline_applications': all_outline_applications,
                'total_processed': total_processed,
                'applications_in_date_range': applications_in_date_range,
                'requests_made': requests_made,
                'data_source': 'UK Government Planning Data API (Smart Processing)',
                'search_period': f'Since {start_date}',
                'completion_status': 'complete',
                'cached_at': None,  # Will be set when cached
                'cache_key': cache_key
            }
            
            # Cache the results for future use (expires in 7 days since planning data changes slowly)
            if use_cache and cache_manager and len(all_outline_applications) > 0:
                print("üíæ Caching outline results for future use...")
                try:
                    # Convert to format expected by cache (list of dicts)
                    cache_data = [results]  # Wrap in list format
                    cache_manager.set(cache_key, cache_data, expiry_hours=168)  # 7 days
                    results['cached_at'] = "just now"
                    print("‚úÖ Results cached successfully!")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to cache results: {e}")
                    # Don't let caching errors break the main functionality
                    pass
            
            return results
            
        except Exception as e:
            print(f"‚ùå Error retrieving outline applications: {str(e)}")
            return {
                'error': f'Failed to retrieve outline applications: {str(e)}',
                'total_outline_applications': len(all_outline_applications) if 'all_outline_applications' in locals() else 0
            }
    
    def search_brownfield_sites(self, min_hectares: float = None, max_hectares: float = None,
                               min_dwellings: int = None, max_dwellings: int = None,
                               deliverable_only: bool = True, planning_status: str = None,
                               location_filter: str = None, limit: int = 100) -> Dict[str, Any]:
        """Search UK brownfield land register for development opportunities"""
        try:
            print(f"üèóÔ∏è SEARCHING BROWNFIELD SITES...")
            print(f"üîç Filters: hectares={min_hectares}-{max_hectares}, dwellings={min_dwellings}-{max_dwellings}, deliverable={deliverable_only}")
            
            # Government brownfield land API
            url = f"{self.gov_base_url}/entity.json"
            params = {
                'dataset': 'brownfield-land',
                'limit': min(limit, 1000),
                'format': 'json'
            }
            
            print(f"üèóÔ∏è üåê API Request: {url}")
            print(f"üèóÔ∏è üìã Params: {params}")
            
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code != 200:
                print(f"üèóÔ∏è ‚ùå API Error: {response.status_code}")
                return {'error': f'Brownfield API returned {response.status_code}'}
            
            data = response.json()
            all_sites = data.get('entities', [])
            total_available = data.get('count', 0)
            
            print(f"üèóÔ∏è ‚úÖ Retrieved {len(all_sites)} sites from {total_available:,} total brownfield sites")
            
            # Client-side filtering
            filtered_sites = []
            
            for site in all_sites:
                # Extract key fields
                hectares = self._safe_float(site.get('hectares'))
                min_dwellings_site = self._safe_int(site.get('minimum-net-dwellings'))
                max_dwellings_site = self._safe_int(site.get('maximum-net-dwellings'))
                deliverable = site.get('deliverable', '').lower()
                status = site.get('planning-permission-status', '')
                address = site.get('site-address', '')
                
                # Apply filters
                if min_hectares and hectares and hectares < min_hectares:
                    continue
                if max_hectares and hectares and hectares > max_hectares:
                    continue
                if min_dwellings and max_dwellings_site and max_dwellings_site < min_dwellings:
                    continue
                if max_dwellings and min_dwellings_site and min_dwellings_site > max_dwellings:
                    continue
                if deliverable_only and deliverable != 'yes':
                    continue
                if planning_status and planning_status.lower() not in status.lower():
                    continue
                if location_filter and location_filter.lower() not in address.lower():
                    continue
                
                # Format site data
                formatted_site = {
                    'reference': site.get('reference', 'N/A'),
                    'name': site.get('name', 'N/A'),
                    'address': address,
                    'hectares': hectares,
                    'min_dwellings': min_dwellings_site,
                    'max_dwellings': max_dwellings_site,
                    'deliverable': deliverable,
                    'planning_status': status,
                    'planning_date': site.get('planning-permission-date', ''),
                    'planning_type': site.get('planning-permission-type', ''),
                    'ownership': site.get('ownership-status', ''),
                    'notes': site.get('notes', ''),
                    'coordinates': site.get('point', ''),
                    'site_plan_url': site.get('site-plan-url', ''),
                    'organisation': site.get('organisation-entity', '')
                }
                
                filtered_sites.append(formatted_site)
            
            print(f"üèóÔ∏è üéØ Found {len(filtered_sites)} sites matching filters from {len(all_sites)} retrieved")
            
            return {
                'total_sites': len(filtered_sites),
                'sites': filtered_sites,
                'total_available': total_available,
                'data_source': 'UK Government Brownfield Register',
                'data_source_type': 'free'
            }
            
        except Exception as e:
            print(f"üèóÔ∏è ‚ùå Brownfield search error: {str(e)}")
            return {'error': f'Brownfield search failed: {str(e)}'}
    
    def _safe_float(self, value):
        """Safely convert to float"""
        try:
            return float(value) if value else None
        except (ValueError, TypeError):
            return None
    
    def _safe_int(self, value):
        """Safely convert to int"""
        try:
            return int(value) if value else None
        except (ValueError, TypeError):
            return None

    def search_planning_data(self, company_name: str = None, postcode: str = None, 
                           application_type: str = None, decision_date: str = None,
                           start_date: str = None, limit: int = 50, local_authority: str = None,
                           planning_status: str = None, development_category: str = None,
                           sort_by: str = None, include_historic: bool = False) -> Dict[str, Any]:
        """Search for planning data with specific filters including date ranges - tries free government sources first"""
        try:
            # Step 1: Try free UK government planning data sources first
            print("üÜì Attempting to fetch from free UK government planning data...")
            
            try:
                # Debug: Check what parameters we're getting
                print(f"üîç SEARCH PARAMS: app_type='{application_type}', start='{start_date}', decision='{decision_date}', postcode='{postcode}', company='{company_name}', limit={limit}")
                
                # Use the existing working government API function with all parameters
                # Pass new parameters to government API
                gov_result = self._try_government_api_fallback(
                    application_type, start_date, decision_date, postcode, company_name, limit,
                    local_authority, planning_status, development_category
                )
                
                if gov_result and gov_result.get('total_applications', 0) > 0:
                    print(f"üèõÔ∏è ‚úÖ Government API SUCCESS: {gov_result['total_applications']} applications")
                    gov_result['data_source'] = 'UK Government (Free)'
                    gov_result['data_source_type'] = 'free'
                else:
                    print("üèõÔ∏è ‚ö†Ô∏è Government API returned no applications")
                    
            except Exception as e:
                print(f"üèõÔ∏è ‚ùå Government API error: {str(e)}")
                gov_result = {'total_applications': 0, 'applications': []}
            
            if gov_result and gov_result.get('total_applications', 0) > 0:
                print(f"‚úÖ Success with free government data: {gov_result['total_applications']} applications")
                gov_result['data_source'] = 'UK Government (Free)'
                gov_result['data_source_type'] = 'free'
                return gov_result
            
            print("‚ö†Ô∏è Government API had no results, trying planning.org.uk...")
            
            # Step 2: Fallback to planning.org.uk for broader coverage
            return self._search_paid_planning_api(
                company_name, postcode, application_type, 
                decision_date, start_date, limit, local_authority
            )
            
        except Exception as e:
            return {'error': f'Planning data search failed: {str(e)}'}
    
    def _try_government_planning_api(self, application_type: str = None, start_date: str = None,
                                   decision_date: str = None, postcode: str = None, 
                                   company_name: str = None, limit: int = 50, local_authority: str = None,
                                   planning_status: str = None, development_category: str = None) -> Dict[str, Any]:
        """Try UK government planning data sources (free)"""
        try:
            # UK Planning Data API - correct endpoint structure (/entity.json)
            gov_url = "https://www.planning.data.gov.uk/entity.json"
            params = {
                'limit': min(limit, 500),  # API rate limits
                'dataset': 'planning-application',  # Filter to planning applications only
                'format': 'json'
            }
            
            # Add typology filter for planning application types (all 41 official types)
            if application_type:
                typology_mapping = {
                    'Householder Planning Permission': 'householder-planning-permission',
                    'Householder Planning and Demolition in Conservation Area': 'householder-planning-and-demolition-in-a-conservation-area',
                    'Householder Planning and Listed Building Consent': 'householder-planning-and-listed-building-consent',
                    'Full Planning Permission': 'full-planning-permission',
                    'Outline Planning Permission ‚Äì Some Matters Reserved': 'outline-planning-permission-‚Äì-some-matters-reserved',
                    'Outline Planning Permission ‚Äì All Matters Reserved': 'outline-planning-permission-‚Äì-all-matters-reserved',
                    'Full Planning and Demolition in Conservation Area': 'full-planning-and-demolition-in-a-conservation-area',
                    'Full Planning and Listed Building Consent': 'full-planning-and-listed-building-consent',
                    'Full Planning and Display of Advertisements': 'full-planning-and-display-of-advertisments',
                    'Demolition in Conservation Area': 'demolition-in-a-conservation-area',
                    'Listed Building Consent': 'listed-building-consent',
                    'Consent to Display Advertisement': 'consent-to-display-an-advertisment',
                    'Lawful Development ‚Äì Existing Use': 'lawful-development-‚Äì-existing-use',
                    'Lawful Development ‚Äì Proposed Use': 'lawful-development-‚Äì-proposed-use',
                    'Building for Agricultural or Forestry Use': 'building-for-agricultural-or-forestry-use',
                    'Private Road for Agricultural or Forestry Use': 'private-road-for-agricultural-or-forestry-use',
                    'Excavation or Deposit Waste for Agriculture': 'excavation-or-deposit-waste-for-agriculture',
                    'Tank or Cage Structure for Use in Fishing Farm': 'tank-or-cage-structure-for-use-in-fishing-farm',
                    'Development for Electronic Communications Network': 'development-for-electronic-communications-network',
                    'Hedgerow Removal Notice': 'hedgerow-removal-notice',
                    'Demolition of Building': 'demolition-of-building',
                    'Removal or Variation of Condition': 'removal-or-variation-of-a-condition',
                    'Approval of Details Reserved by Condition': 'approval-of-details-reserved-by-a-condition',
                    'Tree in Conservation Areas or Subject to Tree Preservation Order': 'tree-in-conservation-areas-or-subject‚Äìto-a-tree-preservation-order',
                    'Non-Material Amendment': 'non-material-amendment',
                    'Larger Home Extension': 'larger-home-extension',
                    'Retail to Restaurant or Cafe': 'retail-to-restaurant-or-cafe',
                    'Takeaway to Dwelling Houses': 'takeaway-or-sui-generis-or-mixed-use-to-dwelling-houses',
                    'Retail to Assembly or Leisure': 'retail-to-assembly-or-leisure',
                    'Agriculture to Dwelling Houses': 'agriculture-to-dwelling-houses',
                    'Agriculture to Flexible Commercial Use': 'agriculture-to-flexible-commercial-use',
                    'Agriculture to State Funded School': 'agriculture-to-state-funded-school',
                    'Commercial/Business/Service/Hotel to State Funded School': 'commercial-or-business-or-service-or-hotel-to-state-funded-school',
                    'Offices to Dwelling Houses': 'offices-to-dwelling-houses',
                    'Light Industrial to Dwelling Houses': 'light-industrial-to-dwelling-houses',
                    'Amusements or Casinos to Dwelling Houses': 'amusements-or-casinos-to-dwelling-houses',
                    'Collection Facility for Shop': 'collection-facility-for-a-shop',
                    'Roof Mounted Solar PV on Non-Domestic Building': 'roof-mounted-solar-pv-on-non-domestic-building',
                    'Temporary School on Previously Vacant Commercial Land': 'temporary-school-on-previously-vacant-commercial-land',
                    'Temporary Use for Commercial Film Making': 'temporary-use-for-commercial-film-making',
                    'Retail or Takeaway to Offices': 'retail-or-takeaway-to-offices',
                    # Legacy mappings for backwards compatibility
                    'Full Planning': 'full-planning-permission',
                    'Outline Planning': 'outline-planning-permission-‚Äì-all-matters-reserved'
                }
                typology_value = typology_mapping.get(application_type, application_type.lower().replace(' ', '-'))
                params['typology'] = typology_value
                print(f"üèõÔ∏è üéØ Using typology filter: {typology_value}")
            
            # API-level filtering doesn't work, so optimize for client-side filtering
            if application_type and 'outline' in application_type.lower():
                # Request larger dataset since outline applications are rare (~1-3% of total)
                params['limit'] = min(limit * 5, 10000)  # 5x more data to find outline applications (API max: 10k)
                print(f"üèõÔ∏è üìä OUTLINE OPTIMIZATION: Requesting {params['limit']} planning applications (5x more) to find outline applications")
                print(f"üèõÔ∏è ‚ö†Ô∏è Note: Government API filtering not supported - using enhanced client-side detection")
            
            # Double-check dataset parameter to avoid wrong data types
            params['dataset'] = 'planning-application'
            print(f"üèõÔ∏è üéØ DATASET VERIFICATION: Confirmed dataset=planning-application (not organizations or corporations)")
            
            # Skip typology filter - we're already targeting dataset=planning-application
            # Adding typology conflicts with the dataset parameter
            print(f"üèõÔ∏è ‚úÖ Targeting planning-application dataset directly (no typology needed)")
            
            # Filter by decision date
            if decision_date:
                try:
                    from datetime import datetime
                    date_obj = datetime.strptime(decision_date, '%Y-%m-%d')
                    params['end_date_year'] = date_obj.year
                    params['end_date_month'] = date_obj.month  
                    params['end_date_day'] = date_obj.day
                    params['end_date_match'] = 'gte'
                except:
                    pass
            
            # Filter by entry date (when data was entered in system) - "since" functionality  
            if start_date:
                try:
                    from datetime import datetime
                    date_obj = datetime.strptime(start_date, '%Y-%m-%d')
                    params['entry_date_year'] = date_obj.year
                    params['entry_date_month'] = date_obj.month
                    params['entry_date_day'] = date_obj.day
                    params['entry_date_match'] = 'since'  # Use 'since' like in working example
                    print(f"üèõÔ∏è ‚úÖ Using entry_date parameters: {date_obj.year}-{date_obj.month:02d}-{date_obj.day:02d}")
                except:
                    pass
            
            # Search by company name if provided
            if company_name:
                params['reference'] = [company_name]
            
            print(f"üèõÔ∏è üîç GOVERNMENT API DEBUG:")
            print(f"üèõÔ∏è üåê URL: {gov_url}")
            print(f"üèõÔ∏è üìã Params: {gov_params}")
            print(f"üèõÔ∏è üéØ Expected: Planning applications with references like '23/00002/FUL'")
            
            print(f"üèõÔ∏è First API attempt params: {params}")
            
            # Use planning.org.uk API for real data
            search_url = f"{self.base_url}/search"
            
            if not self.api_key:
                return {
                    'error': 'Planning API key required for real data access.',
                    'help': 'Please generate an API key using your email address in the sidebar.',
                    'total_applications': 0,
                    'applications': []
                }
            
            search_params = {
                'key': self.api_key,
                'return_data': 1,  # Get actual data instead of just counts
                'limit': min(limit, 50)  # Start with smaller limit
            }
            
            # Only request full data if user explicitly wants it and has credits
            if hasattr(self, '_full_data_mode') and self._full_data_mode:
                search_params['return_data'] = 1
            
            print(f"Government API URL: {gov_url}")
            print(f"Government API Params: {params}")
            
            response = self.session.get(gov_url, params=params, timeout=10)
            print(f"Government API Response Status: {response.status_code}")
            
            if response.status_code == 200:
                json_data = response.json()
                result = self._process_government_api_response(json_data)
                return result
            else:
                print(f"Government API failed: {response.status_code}")
                return {'total_applications': 0, 'applications': []}
                
        except Exception as e:
            print(f"üèõÔ∏è ‚ùå Government API exception: {str(e)}")
            return {'total_applications': 0, 'applications': []}
    
    def _search_paid_planning_api(self, company_name: str = None, postcode: str = None, 
                                application_type: str = None, decision_date: str = None,
                                start_date: str = None, limit: int = 50, local_authority: str = None) -> Dict[str, Any]:
        """Search planning.org.uk API (paid/credit-based)"""
        try:
            search_url = f"{self.base_url}/search"
            
            if not self.api_key:
                return {
                    'error': 'No free government data available for your search criteria.',
                    'help': 'Please generate a planning.org.uk API key for broader data access.',
                    'total_applications': 0,
                    'applications': []
                }
            
            search_params = {
                'key': self.api_key,
                'return_data': 1,  # Get actual data instead of just counts
                'limit': min(limit, 50)
            }
            
            # Add date filters
            if start_date:
                search_params['date_from'] = start_date
            if decision_date:
                search_params['date_to'] = decision_date
            
            # Add location filters
            if postcode:
                search_params['postcode'] = postcode
            if company_name:
                search_params['agent'] = company_name
                
            print(f"Planning.org.uk API URL: {search_url}")
            print(f"Planning.org.uk API Params: {search_params}")
            
            response = self.session.get(
                search_url,
                params=search_params,
                timeout=15
            )
            
            print(f"Planning.org.uk API Response Status: {response.status_code}")
            print(f"Planning.org.uk API Response URL: {response.url}")
            print(f"Planning.org.uk API Response Text: {response.text[:500]}...")
            
            time.sleep(self.rate_limit_delay)
            
            if response.status_code == 200:
                try:
                    json_response = response.json()
                    print(f"Planning.org.uk API JSON Keys: {list(json_response.keys()) if json_response else 'No data'}")
                    print(f"Planning.org.uk API Full Response: {json_response}")
                    
                    result = self._process_real_planning_data(json_response)
                    result['data_source'] = 'Planning.org.uk (Paid)'
                    result['data_source_type'] = 'paid'
                    
                    # If no applications found, provide helpful message
                    if result.get('total_applications', 0) == 0:
                        return {
                            'total_applications': 0,
                            'applications': [],
                            'info': 'No planning applications found matching your search criteria. Try adjusting your date range or removing filters.',
                            'data_source': 'Planning.org.uk (Paid)',
                            'data_source_type': 'paid'
                        }
                    
                    return result
                    
                except ValueError as e:
                    print(f"JSON parsing error: {str(e)}")
                    return {
                        'error': f'Invalid JSON response from planning API: {str(e)}',
                        'total_applications': 0,
                        'applications': []
                    }
            elif response.status_code == 401:
                return {
                    'error': 'Invalid API key. Please generate a new key using your email address.',
                    'help': 'Use the "Generate New API Key" section in the sidebar.',
                    'total_applications': 0,
                    'applications': []
                }
            elif response.status_code == 402:
                return {
                    'error': 'Insufficient credits in your planning.org.uk account.',
                    'help': 'Contact planning.org.uk to top up your account credits.',
                    'total_applications': 0,
                    'applications': []
                }
            elif response.status_code == 429:
                return {
                    'error': 'Rate limit exceeded. Please wait a moment and try again.',
                    'total_applications': 0,
                    'applications': []
                }
            else:
                print(f"Planning API Error Response: {response.text[:500]}")
                
                # Provide helpful guidance for 500 errors
                if response.status_code == 500:
                    # Check if this is Birmingham with no recent data
                    if local_authority and 'birmingham' in local_authority.lower() and start_date and start_date >= '2022-01-01':
                        help_text = "üí° **Try older dates**: Birmingham has limited recent planning data. Try searching from 2020 or 2021 for better results."
                    else:
                        help_text = "The planning.org.uk server is temporarily unavailable. Please try again later or try a different date range."
                    
                    return {
                        'error': f'Planning API server error ({response.status_code})',
                        'help': help_text,
                        'details': response.text[:200] if response.text else 'No additional details',
                        'total_applications': 0,
                        'applications': []
                    }
                
                return {
                    'error': f'Planning API error: {response.status_code}',
                    'details': response.text[:200] if response.text else 'No additional details',
                    'total_applications': 0,
                    'applications': []
                }
                
        except Exception as e:
            return {'error': f'Planning data search failed: {str(e)}'}
    
    def _process_real_planning_data(self, raw_data: Dict) -> Dict[str, Any]:
        """Process real planning.org.uk API response"""
        try:
            print(f"üîç PROCESSING API RESPONSE: {raw_data}")
            
            response_data = raw_data.get('response', {})
            api_status = response_data.get('status', 'Unknown')
            print(f"üîç EXTRACTED STATUS: '{api_status}'")
            
            # Handle credit/payment issues OR count-only mode (return_data=0)
            if api_status == 'ERROR':
                credits_info = response_data.get('credits', {})
                remaining_credits = credits_info.get('remaining', 'Unknown')
                application_count = response_data.get('application_count', 0)
                
                if 'Insufficient credits' in response_data.get('message', ''):
                    return {
                        'total_applications': application_count,
                        'applications': [],
                        'api_status': 'Success (Count Mode)',
                        'api_message': f'Found {application_count:,} matching applications',
                        'credit_info': f'Your search found {application_count:,} planning applications, but you need credits to view details.',
                        'preview_mode': True
                    }
            elif api_status == 'OK':
                # Handle successful count-only mode (return_data=0)
                application_count = response_data.get('application_count', 0)
                print(f"üîç OK STATUS - Application count: {application_count}")
                if application_count > 0:
                    print(f"‚úÖ EARLY SUCCESS RETURN: Found {application_count} applications!")
                    return {
                        'total_applications': application_count,
                        'applications': [],
                        'api_status': 'Success (Count Mode)',
                        'api_message': f'Found {application_count:,} matching applications',
                        'credit_info': f'Your search found {application_count:,} planning applications. Add credits to view full details.',
                        'preview_mode': True,
                        'data_source': 'Planning.org.uk (Count Mode)',
                        'data_source_type': 'count'
                    }
            
            results = response_data.get('results', [])
            
            # Handle different response formats
            if not results and 'data' in response_data:
                results = response_data.get('data', [])
            
            # Handle case where results is a dict instead of list
            if isinstance(results, dict):
                results = [results]
            
            # Check if this is count-only mode (return_data=0) with OK status
            application_count = response_data.get('application_count', len(results))
            
            planning_summary = {
                'total_applications': max(len(results), application_count),
                'applications': [],
                'api_status': response_data.get('status', 'Unknown'),
                'api_message': response_data.get('message', '')
            }
            
            print(f"Found {len(results)} detailed applications in API response")
            print(f"Total application count from API: {application_count}")
            print(f"API Status: '{api_status}'")
            print(f"Checking conditions: api_status == 'OK': {api_status == 'OK'}, application_count > 0: {application_count > 0}")
            
            # Handle count-only mode for OK status - triggers when we have application_count but no detailed results
            if api_status == 'OK' and application_count > 0:
                print(f"‚úÖ SUCCESS: Found {application_count} applications in count mode!")
                result = {
                    'total_applications': application_count,
                    'applications': [],
                    'api_status': 'Success (Count Mode)',
                    'api_message': f'Found {application_count:,} matching applications',
                    'credit_info': f'Your search found {application_count:,} planning applications. Add credits to view full details.',
                    'preview_mode': True,
                    'data_source': 'Planning.org.uk (Count Mode)',
                    'data_source_type': 'count'
                }
                print(f"Returning result: {result}")
                return result
            else:
                print(f"‚ùå CONDITION NOT MET: api_status='{api_status}', application_count={application_count}")
            
            # Process real planning applications from API
            for i, result in enumerate(results):
                print(f"Processing application {i+1}: {result}")
                
                app_data = {
                    'application_type': result.get('application_type', result.get('development_type', result.get('type', 'N/A'))),
                    'decision_date': result.get('decision_date', result.get('determination_date', result.get('decided', 'N/A'))),
                    'name': result.get('proposal', result.get('description', result.get('title', 'N/A'))),
                    'reference': result.get('application_no', result.get('reference', result.get('app_ref', 'N/A'))),
                    'description': result.get('proposal', result.get('description', result.get('details', 'N/A'))),
                    'applicant': result.get('applicant_name', result.get('applicant', result.get('agent', 'N/A'))),
                    'start_date': result.get('application_received_date', result.get('received_date', result.get('submitted', 'N/A'))),
                    'organisation': result.get('local_planning_authority', result.get('council', result.get('authority', 'N/A'))),
                    'status': result.get('application_status', result.get('status', result.get('state', 'N/A'))),
                    'point': f"POINT({result.get('longitude', 0)} {result.get('latitude', 0)})" if result.get('longitude') else 'N/A'
                }
                planning_summary['applications'].append(app_data)
            
            return planning_summary
            
        except Exception as e:
            print(f"Error processing planning data: {str(e)}")
            return {
                'error': f'Failed to process planning data: {str(e)}',
                'total_applications': 0,
                'applications': []
            }
    
    def _try_government_api_fallback(self, application_type: str = None, start_date: str = None, 
                                   decision_date: str = None, postcode: str = None,
                                   company_name: str = None, limit: int = 50, local_authority: str = None,
                                   planning_status: str = None, development_category: str = None) -> Dict[str, Any]:
        """Try government planning data API with real endpoints and all filters"""
        try:
            print(f"üèõÔ∏è RECEIVED PARAMS: app_type='{application_type}', start='{start_date}', decision='{decision_date}', postcode='{postcode}', company='{company_name}', limit={limit}, local_authority='{local_authority}'")
            print(f"üèõÔ∏è ‚¨ÜÔ∏è API LIMIT INCREASED: Will request up to {min(limit, 1000)} records from government API (was limited to 200)")
            
            # Try the government API - use correct endpoint structure from user
            gov_url = "https://www.planning.data.gov.uk/entity.json"
            gov_params = {
                'limit': min(limit, 1000),  # Increase to 1000 to support larger searches
                'dataset': 'planning-application',  # Target actual planning applications
            }
            
            # Add local authority filter if specified
            if local_authority:
                # Strip status indicators from authority name
                clean_authority = local_authority.replace("‚úÖ ", "").replace(" (Limited Data)", "").strip()
                print(f"üèõÔ∏è üéØ LOCAL AUTHORITY FILTER: Looking for '{clean_authority}' applications")
                
                # Map local authority names to their numeric entity IDs in the government API
                authority_mappings = {
                    'Birmingham City Council': None,  # Entity 44 has no data, let it search all
                    'Leeds City Council': None,       # Entity 195 uncertain, let it search all
                    'Liverpool City Council': None,   # Entity 202 uncertain, let it search all
                    'Sheffield City Council': None,   # Entity 294 uncertain, let it search all
                    'Coventry City Council': None,    # Entity 96 uncertain, let it search all
                    'Bristol City Council': None,     # Entity 66 uncertain, let it search all
                    'Newcastle City Council': None,   # Entity 228 uncertain, let it search all
                    'Manchester City Council': None,  # To be found
                    'Nottingham City Council': None,  # To be found
                    'Leicester City Council': None,   # To be found
                    'Bradford Council': None,         # To be found
                    'Cherwell District Council': 109, # Known working
                    'Oxford City Council': None       # To be found
                }
                
                authority_id = authority_mappings.get(clean_authority)
                if authority_id is not None:
                    gov_params['organisation_entity'] = [authority_id]  # API expects array of integers
                    print(f"üèõÔ∏è üìç AUTHORITY MAPPED: {clean_authority} ‚Üí [{authority_id}] (array format)")
                else:
                    print(f"üèõÔ∏è ‚ö†Ô∏è AUTHORITY NOT MAPPED: {clean_authority} - will search all authorities")
            
            # Optimize data retrieval for outline application searches  
            if application_type and 'outline' in application_type.lower():
                # Since outline applications are rare, request much larger dataset for better results
                gov_params['limit'] = min(limit * 5, 10000)  # 5x more data to find outlines (API max: 10k)
                print(f"üèõÔ∏è üìä OUTLINE SEARCH ENHANCEMENT: Requesting {gov_params['limit']} planning applications (5x more data)")
                print(f"üèõÔ∏è üîç Strategy: Search larger dataset since outline applications are only ~1-3% of total")
            
            # Ensure we're definitely targeting planning applications, not other datasets
            gov_params['dataset'] = 'planning-application'
            print(f"üèõÔ∏è ‚úÖ DATASET LOCK: Ensuring dataset=planning-application (not development-corporation or government-organisation)")
            
            # Always add date filters (required by government API)
            if not start_date or start_date.strip() == 'None':
                actual_start_date = '2020-01-01'  # Go back for maximum coverage
                print(f"üèõÔ∏è üîç NO DATE PROVIDED: Using broad range from {actual_start_date} for maximum coverage")
            elif start_date and start_date.strip():
                actual_start_date = start_date
                # Check for future dates and adjust
                from datetime import datetime
                today = datetime.now().date().isoformat()
                if start_date > today:
                    actual_start_date = '2020-01-01'  # Use historical data for future dates
                    print(f"üèõÔ∏è ‚ö†Ô∏è FUTURE DATE DETECTED: {start_date} is in the future, using {actual_start_date} instead")
                else:
                    # Use the provided date as-is since we're in 2025
                    print(f"üèõÔ∏è ‚úÖ Using provided date: {start_date}")
            
            # Convert to government API format - use entry_date parameters
            year, month, day = actual_start_date.split('-')
            gov_params.update({
                'entry_date_year': int(year),
                'entry_date_month': int(month), 
                'entry_date_day': int(day),
                'entry_date_match': 'since'
            })
            print(f"üèõÔ∏è ‚úÖ Using date parameters: {actual_start_date} (since)")
                
            if decision_date and decision_date.strip():
                # Convert YYYY-MM-DD to government API format  
                year, month, day = decision_date.split('-')
                gov_params.update({
                    'end_date_year': int(year),
                    'end_date_month': int(month),
                    'end_date_day': int(day), 
                    'end_date_match': 'since'
                })
                print(f"üèõÔ∏è ‚úÖ Adding decision date filter: {decision_date} (since)")
            
            # Add application type filtering if specified
            if application_type and application_type.strip():
                # Map common application types to government API format
                app_type_mappings = {
                    'Full Planning Permission': 'planning-application',
                    'Outline Planning Permission': 'outline-planning-application',
                    'Advertisement Consent': 'advertisement-consent',
                    'Listed Building Consent': 'listed-building-consent'
                }
                
                api_app_type = app_type_mappings.get(application_type, 'planning-application')
                gov_params['typology'] = api_app_type
                print(f"üèõÔ∏è ‚úÖ Adding application type filter: {application_type} ‚Üí {api_app_type}")
            else:
                print(f"üèõÔ∏è ‚ÑπÔ∏è No application type specified - showing all types")
            
            if company_name and company_name.strip():
                # Search by reference for company names
                gov_params['reference'] = [company_name]
                print(f"üèõÔ∏è ‚úÖ Adding company reference filter: {company_name}")
                
            # Note: Postcode filtering requires geographic coordinates
            # For now, we'll handle postcodes client-side
            
            # Remove any hardcoded authority restrictions to get diverse data
            # Note: The API seems to be defaulting to authority 109 (Cherwell) 
            # Let's try without authority restrictions to get broader coverage
            
            print(f"üèõÔ∏è Trying government API: {gov_url}")
            print(f"üèõÔ∏è Government params: {gov_params}")
            print(f"üèõÔ∏è üîç DATASET CHECK: {gov_params.get('dataset')} (should be planning-application)")
            print(f"üèõÔ∏è üöÄ ATTEMPTING UNRESTRICTED SEARCH (all UK authorities)")
            
            response = self.session.get(gov_url, params=gov_params, timeout=10)
            print(f"üèõÔ∏è Government API response: {response.status_code}")
            
            if response.status_code == 200:
                json_data = response.json()
                entities = json_data.get('entities', [])
                print(f"üèõÔ∏è Government data received: {len(entities)} entities")
                print(f"üèõÔ∏è üîç RESPONSE DEBUG: First 200 chars: {str(response.text)[:200]}")
                print(f"üèõÔ∏è üîç JSON KEYS: {list(json_data.keys()) if json_data else 'No JSON data'}")
                if entities and len(entities) > 0:
                    print(f"üèõÔ∏è üîç FIRST ENTITY: {entities[0] if entities else 'No entities'}")
                else:
                    print(f"üèõÔ∏è ‚ö†Ô∏è NO ENTITIES FOUND in response despite API returning data")
                
                # Always try to get more diverse data by making additional API calls
                # Check what authorities we have in current results  
                authorities = set()
                for entity in entities[:10]:  # Check first 10
                    auth_id = entity.get('organisation-entity', 'unknown')
                    authorities.add(auth_id)
                print(f"üèõÔ∏è üîç AUTHORITY DIVERSITY CHECK: Found {len(authorities)} different authorities in first 10 results: {authorities}")
                
                # Force diversity by always trying additional calls
                print(f"üèõÔ∏è üöÄ FORCING AUTHORITY DIVERSITY: Making additional API calls for better coverage")
                
                # If we only got one authority, try to get more diverse data
                all_entities = entities.copy()
                if len(authorities) == 1 and len(entities) > 0:
                    print(f"üèõÔ∏è üö® SINGLE AUTHORITY DETECTED: All results from authority {list(authorities)[0]}")
                    print(f"üèõÔ∏è üîÑ TRYING MULTIPLE API CALLS to get diverse authority data...")
                    
                    # Try different offset values to get different authority chunks
                    for offset in [50, 100, 150, 200]:
                        try:
                            diverse_params = gov_params.copy()
                            diverse_params['limit'] = 25  # Smaller chunks
                            diverse_params['offset'] = offset
                            print(f"üèõÔ∏è üìä Trying offset {offset} to find different authorities...")
                            
                            diverse_response = self.session.get(gov_url, params=diverse_params, timeout=5)
                            if diverse_response.status_code == 200:
                                diverse_data = diverse_response.json()
                                diverse_entities = diverse_data.get('entities', [])
                                if diverse_entities:
                                    # Check if these are from different authorities
                                    new_authorities = set()
                                    for entity in diverse_entities[:5]:
                                        auth_id = entity.get('organisation-entity', 'unknown')
                                        new_authorities.add(auth_id)
                                    
                                    print(f"üèõÔ∏è üìã Offset {offset}: Found authorities {new_authorities}")
                                    if new_authorities != authorities:
                                        print(f"üèõÔ∏è ‚úÖ FOUND DIVERSE DATA: Adding {len(diverse_entities)} applications from different authorities")
                                        all_entities.extend(diverse_entities)
                                        authorities.update(new_authorities)
                                        
                                        # Stop if we have good diversity
                                        if len(authorities) >= 3:
                                            print(f"üèõÔ∏è üéØ SUFFICIENT DIVERSITY: Now have {len(authorities)} different authorities")
                                            break
                        except Exception as e:
                            print(f"üèõÔ∏è ‚ö†Ô∏è Offset {offset} failed: {e}")
                            continue
                    
                    print(f"üèõÔ∏è üìä FINAL DIVERSITY: {len(authorities)} authorities, {len(all_entities)} total applications")
                
                if all_entities:
                    # Process the combined diverse data
                    combined_data = {'entities': all_entities}
                    result = self._process_government_api_response(combined_data)
                    
                    # Apply client-side filtering using the adjusted date we actually used for API
                    applications = result.get('applications', [])
                    filtered_apps = self._apply_client_side_date_filtering(
                        applications, actual_start_date, decision_date, postcode, company_name, application_type
                    )
                    
                    result['applications'] = filtered_apps[:limit]  # Respect limit
                    result['total_applications'] = len(filtered_apps)
                    
                    print(f"üèõÔ∏è ‚úÖ Real government data after filtering: {len(filtered_apps)} applications")
                    print(f"üèõÔ∏è üéØ Date filter applied - showing applications from {start_date or 'any date'} onwards")
                    return result
                else:
                    print("üèõÔ∏è Government API returned no entities")
                    return {'total_applications': 0, 'applications': []}
            else:
                print(f"üèõÔ∏è Filtered government API failed: {response.status_code} - {response.text[:200]}")
                # Try with basic filters if complex call failed
                print("üèõÔ∏è Trying government API with basic filters...")
                
                # Calculate correct limit (including outline enhancement)
                basic_limit = limit
                if application_type and 'outline' in application_type.lower():
                    basic_limit = min(limit * 5, 10000)  # Keep 5x enhancement for outline searches (API max: 10k)
                
                # Try completely different approach: search without any restrictions
                # to see if we can get data from multiple authorities
                    print(f"üèõÔ∏è üìä FALLBACK OUTLINE ENHANCEMENT: Requesting {basic_limit} planning applications (5x more data)")
                
                simple_params = {
                    'limit': basic_limit,
                    'dataset': 'planning-application',  # Ensure we still target planning applications
                    'format': 'json'
                }
                
                # Skip application type filtering for government API - not supported
                # Will filter client-side instead
                    
                print(f"üèõÔ∏è üîÑ SIMPLE FALLBACK API Call: {gov_url}")
                print(f"üèõÔ∏è üìã Simple params: {simple_params}")
                print(f"üèõÔ∏è üîç SIMPLE DATASET CHECK: {simple_params.get('dataset')} (should be planning-application)")
                
                simple_response = self.session.get(gov_url, params=simple_params, timeout=10)
                print(f"üèõÔ∏è Simple fallback response: {simple_response.status_code}")
                
                if simple_response.status_code == 200:
                    simple_data = simple_response.json()
                    entities = simple_data.get('entities', [])
                    if entities:
                        print(f"üèõÔ∏è ‚úÖ Simple government API worked: {len(entities)} entities")
                        result = self._process_government_api_response(simple_data)
                        
                        # Apply client-side filtering to fallback results using actual dates
                        applications = result.get('applications', [])  
                        filtered_apps = self._apply_client_side_date_filtering(
                            applications, actual_start_date if 'actual_start_date' in locals() else start_date, 
                            decision_date, postcode, company_name, application_type
                        )
                        
                        result['applications'] = filtered_apps[:limit]
                        result['total_applications'] = len(filtered_apps)
                        
                        print(f"üèõÔ∏è ‚úÖ Fallback data after filtering: {len(filtered_apps)} applications")
                        return result
                
                print("üèõÔ∏è All government API attempts failed")
                return {'total_applications': 0, 'applications': []}
                
        except Exception as e:
            print(f"üèõÔ∏è Government API exception: {str(e)}")
            return self._get_demo_planning_data(application_type)
    
    def _apply_client_side_date_filtering(self, applications: List[Dict], start_date: str = None, 
                                        decision_date: str = None, postcode: str = None, 
                                        company_name: str = None, application_type: str = None) -> List[Dict]:
        """Apply client-side filtering since government API ignores our filters"""
        filtered_apps = applications.copy()
        
        try:
            # Apply start date filtering properly, but don't filter out all results
            if start_date:
                print(f"üèõÔ∏è üîç Applying start date filter: {start_date}")
                initial_count = len(filtered_apps)
                date_filtered = [
                    app for app in filtered_apps 
                    if app.get('start_date', '2023-01-01') >= start_date or app.get('decision_date', '2023-01-01') >= start_date
                ]
                
                # If date filtering removes all results, be more flexible
                if len(date_filtered) == 0 and initial_count > 0:
                    print(f"üèõÔ∏è ‚ö†Ô∏è STRICT DATE FILTER removed all {initial_count} applications")
                    print(f"üèõÔ∏è üí° RELAXING FILTER: Showing all available applications instead of none")
                    # Keep original applications instead of returning empty list
                    print(f"üèõÔ∏è üîç After flexible date filter: {len(filtered_apps)} applications (kept all)")
                else:
                    filtered_apps = date_filtered
                    print(f"üèõÔ∏è üîç After start date filter: {len(filtered_apps)} applications (was {initial_count})")
                    sample_dates = [app.get('start_date', 'N/A') for app in filtered_apps[:3]]
                    print(f"üèõÔ∏è üîç Sample start dates: {sample_dates}")
            
            # Filter by decision date
            if decision_date:
                print(f"üèõÔ∏è üîç Applying decision date filter: {decision_date}")
                filtered_apps = [
                    app for app in filtered_apps 
                    if app.get('decision_date') and app.get('decision_date') >= decision_date
                ]
                print(f"üèõÔ∏è üîç After decision date filter: {len(filtered_apps)} applications")
            
            # Filter by postcode (basic matching)
            if postcode and postcode.strip():
                print(f"üèõÔ∏è üîç Applying postcode filter: {postcode}")
                postcode_upper = postcode.upper()
                filtered_apps = [
                    app for app in filtered_apps 
                    if any(postcode_upper in str(app.get(field, '')).upper() 
                          for field in ['point', 'name', 'description'])
                ]
                print(f"üèõÔ∏è üîç After postcode filter: {len(filtered_apps)} applications")
            
            # Filter by company name
            if company_name and company_name.strip():
                print(f"üèõÔ∏è üîç Applying company name filter: {company_name}")
                company_upper = company_name.upper()
                filtered_apps = [
                    app for app in filtered_apps 
                    if company_upper in str(app.get('applicant', '')).upper()
                    or company_upper in str(app.get('name', '')).upper()
                ]
                print(f"üèõÔ∏è üîç After company name filter: {len(filtered_apps)} applications")
            
            # Show what application types are actually available
            if applications:
                available_types = set()
                for app in filtered_apps[:5]:  # Check first 5 apps
                    app_type = app.get('application_type', 'N/A')
                    available_types.add(app_type)
                # Show all available application types with counts
                all_app_types = sorted(list(available_types))
                print(f"üèõÔ∏è üìã ALL PLANNING APPLICATION CATEGORIES ({len(all_app_types)} types):")
                for i, app_type in enumerate(all_app_types, 1):
                    count = len([app for app in filtered_apps if str(app.get('application_type', 'Unknown')) == app_type])
                    print(f"üèõÔ∏è üìã {i}. {app_type} ({count} applications)")
                print(f"üèõÔ∏è üìã TOTAL: {len(all_app_types)} different planning categories in current dataset")
                
                # Debug: Check all fields that might contain outline info
                sample_refs = [str(app.get('reference', 'No-ref')) for app in filtered_apps[:5]]
                print(f"üèõÔ∏è üîç SAMPLE REFERENCE FIELDS: {sample_refs}")
                
                # Check for outline in the actual application_type field
                outline_in_types = [app for app in filtered_apps if 'outline' in str(app.get('application_type', '')).lower()]
                print(f"üèõÔ∏è üéØ APPLICATIONS WITH 'OUTLINE' IN TYPE FIELD: {len(outline_in_types)} found")
                
                # Check planning application status for outline-related statuses
                outline_statuses = [app for app in filtered_apps if any(term in str(app.get('planning-application-status', '')).lower() for term in ['outline', 'reserved'])]
                print(f"üèõÔ∏è üéØ APPLICATIONS WITH OUTLINE/RESERVED IN STATUS: {len(outline_statuses)} found")
                
                # Show total potentially missed applications
                total_potential = len(outline_in_types) + len(outline_statuses)
                print(f"üèõÔ∏è üìä TOTAL POTENTIAL OUTLINE APPLICATIONS: {total_potential} (may include duplicates)")
                
                # Check if we need broader date range for outline applications
                if not any('outline' in app_type.lower() for app_type in all_app_types):
                    print(f"üèõÔ∏è üí° NO OUTLINE APPLICATIONS in current date range - try broader dates like 2020-2025 for more variety")
            
            # Filter by application type with flexible matching  
            if application_type and application_type.strip():
                print(f"üèõÔ∏è üîç Applying flexible application type filter: {application_type}")
                app_type_lower = application_type.lower()
                
                # Try multiple matching strategies
                original_count = len(filtered_apps)
                print(f"üèõÔ∏è üìä Starting with {original_count} applications before type filtering")
                
                # For very common types, be less restrictive and show more results
                if 'full planning permission' in app_type_lower and 'outline' not in app_type_lower:
                    # Show all full planning types - most common category
                    common_full_matches = [
                        app for app in filtered_apps 
                        if any(term in str(app.get('application_type', '')).lower() 
                               for term in ['full planning permission', 'householder planning permission', 'listed building consent'])
                    ]
                    print(f"üèõÔ∏è üìà Found {len(common_full_matches)} applications for broad 'Full Planning' search")
                    filtered_apps = common_full_matches[:limit]
                    print(f"üèõÔ∏è ‚úÖ Real government data after broad filtering: {len(filtered_apps)} applications")
                    return filtered_apps
                
                # For outline searches, don't be overly restrictive - show reasonable number
                elif 'outline' in app_type_lower:
                    print(f"üèõÔ∏è üîç OUTLINE SEARCH: Will try to get more than the usual 5 results")
                    # Continue with existing outline logic but increase limits
                
                # For specific searches like "outline", use targeted filtering
                # For broader searches, be less restrictive
                if 'outline' in app_type_lower:
                    # Strategy 1: Official planning-application-category field (most reliable)
                    category_matches = [
                        app for app in filtered_apps 
                        if 'outline' in str(app.get('planning-application-category', '')).lower()
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(category_matches)} applications with 'outline' in planning-application-category field")
                elif 'full' in app_type_lower:
                    category_matches = [
                        app for app in filtered_apps 
                        if 'full' in str(app.get('planning-application-category', '')).lower() or
                           'full' in str(app.get('application_type', '')).lower()
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(category_matches)} applications with 'full' in category/type fields")
                    
                    # For "Full Planning Permission", be less restrictive - include most common types
                    if len(category_matches) < 10:
                        common_full_types = [
                            'Full Planning Permission', 'Full Planning Permission (Major)', 
                            'Householder Planning Permission', 'Listed Building Consent'
                        ]
                        for app in filtered_apps:
                            app_type = str(app.get('application_type', ''))
                            if any(common_type in app_type for common_type in common_full_types):
                                if app not in category_matches:
                                    category_matches.append(app)
                        print(f"üèõÔ∏è üìà Enhanced to {len(category_matches)} applications by including common planning types")
                
                else:
                    # For other specific application types, try exact matching first
                    category_matches = [
                        app for app in filtered_apps 
                        if app_type_lower in str(app.get('application_type', '')).lower()
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(category_matches)} applications with exact type matching")
                
                # Strategy 1b: Also check planning-application-type field
                type_matches = []
                if 'outline' in app_type_lower:
                    type_matches = [
                        app for app in filtered_apps 
                        if 'outline' in str(app.get('planning-application-type', '')).lower()
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(type_matches)} applications with 'outline' in planning-application-type field")
                
                # Strategy 2: Exact match
                exact_matches = [
                    app for app in filtered_apps 
                    if app_type_lower == str(app.get('application_type', '')).lower()
                ]
                
                # Strategy 2: Contains match in application_type
                contains_matches = [
                    app for app in filtered_apps 
                    if app_type_lower in str(app.get('application_type', '')).lower()
                ]
                
                # Strategy 2b: Enhanced outline detection across multiple fields
                if 'outline' in app_type_lower:
                    # Check application_type field
                    outline_type_matches = [
                        app for app in filtered_apps
                        if 'outline' in str(app.get('application_type', '')).lower()
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(outline_type_matches)} applications with 'outline' in application_type field")
                    
                    # Check planning-application-status field for outline/reserved
                    outline_status_matches = [
                        app for app in filtered_apps
                        if any(term in str(app.get('planning-application-status', '')).lower() 
                               for term in ['outline', 'reserved'])
                    ]
                    print(f"üèõÔ∏è üéØ Found {len(outline_status_matches)} applications with outline/reserved in status field")
                    
                    # Combine all matches
                    all_outline_matches = outline_type_matches + outline_status_matches
                    contains_matches.extend(all_outline_matches)
                    
                    # Remove duplicates by entity ID
                    seen_entities = set()
                    unique_contains = []
                    for app in contains_matches:
                        entity_id = app.get('entity', str(app))
                        if entity_id not in seen_entities:
                            seen_entities.add(entity_id)
                            unique_contains.append(app)
                    contains_matches = unique_contains
                    
                    print(f"üèõÔ∏è üìä TOTAL UNIQUE OUTLINE MATCHES (type + status + contains): {len(unique_contains)}")
                
                # Strategy 3: Contains match in description (for "outline application")  
                description_matches = []
                if 'outline' in app_type_lower:
                    print(f"üèõÔ∏è üîç Searching {len(filtered_apps)} applications for 'outline' in descriptions...")
                    outline_count = 0
                    for i, app in enumerate(filtered_apps):
                        description = str(app.get('description', '')).lower()
                        # Show sample descriptions but not all 50+ to avoid spam
                        if i < 10 or 'outline' in description:
                            print(f"üèõÔ∏è üìã App {i+1}: {description[:100]}")
                        
                        # More robust outline detection
                        if ('outline' in description or 
                            'outline application' in description or 
                            'outline planning' in description or
                            'outline consent' in description or
                            'outline permission' in description or
                            'reserved matters' in description or
                            description.startswith('outline') or
                            'outline development' in description):
                            description_matches.append(app)
                            outline_count += 1
                            print(f"üèõÔ∏è üìù ‚úÖ FOUND OUTLINE #{outline_count}: {description[:150]}")
                    
                    print(f"üèõÔ∏è üîç Description search found {len(description_matches)} outline applications from {len(filtered_apps)} total")
                    
                    # If no outline found in this batch, expand search
                    if len(description_matches) == 0:
                        print(f"üèõÔ∏è üö® No outline applications found in current batch - this may be a different data set")
                        print(f"üèõÔ∏è üí° Try adjusting date ranges or search parameters to find outline applications")
                
                # Strategy 4: Partial word match
                partial_matches = [
                    app for app in filtered_apps 
                    if any(word in str(app.get('application_type', '')).lower() 
                          for word in app_type_lower.split('-'))
                ]
                
                # Check if we found matches
                matches_found = (exact_matches or description_matches or 
                               contains_matches or partial_matches)
                
                if matches_found:
                    # Show match details for user awareness
                    if exact_matches:
                        print(f"üèõÔ∏è ‚úÖ Found {len(exact_matches)} exact matches")
                    if description_matches:
                        print(f"üèõÔ∏è ‚úÖ Found {len(description_matches)} outline applications from descriptions")
                    if contains_matches:
                        print(f"üèõÔ∏è ‚úÖ Found {len(contains_matches)} contains matches")
                    if partial_matches:
                        print(f"üèõÔ∏è ‚úÖ Found {len(partial_matches)} partial matches")
                    
                    # Use the best matching strategy - prioritize official category fields
                    if category_matches:
                        filtered_apps = category_matches
                        print(f"üèõÔ∏è ‚úÖ Using {len(category_matches)} applications with official planning-application-category field")
                    elif type_matches:
                        filtered_apps = type_matches
                        print(f"üèõÔ∏è ‚úÖ Using {len(type_matches)} applications with planning-application-type field")
                    elif description_matches:  # Prioritize description matches for outline planning
                        filtered_apps = description_matches
                        print(f"üèõÔ∏è ‚úÖ Using {len(description_matches)} outline applications from descriptions")
                    elif contains_matches:
                        filtered_apps = contains_matches  
                        print(f"üèõÔ∏è ‚úÖ Using {len(contains_matches)} contains matches")
                    elif exact_matches:
                        filtered_apps = exact_matches
                        print(f"üèõÔ∏è ‚úÖ Using {len(exact_matches)} exact matches")
                    elif partial_matches:
                        filtered_apps = partial_matches
                        print(f"üèõÔ∏è ‚úÖ Using {len(partial_matches)} partial matches")
                else:
                    print(f"üèõÔ∏è ‚ö†Ô∏è No matches found for '{application_type}' - showing all {original_count} applications")
                    # Don't filter - show all applications
                
                print(f"üèõÔ∏è üîç After flexible application type filter: {len(filtered_apps)} applications")
                
            return filtered_apps
            
        except Exception as e:
            print(f"üèõÔ∏è ‚ö†Ô∏è Filter error: {e}")
            return applications  # Return unfiltered if filtering fails
    
    def _process_government_api_response(self, raw_data: Dict) -> Dict[str, Any]:
        """Process government planning data API response"""
        try:
            entities = raw_data.get('entities', [])
            
            # Debug: Show what's actually in the first entity
            if entities:
                print(f"üèõÔ∏è üîç DEBUG: Sample entity keys: {list(entities[0].keys())}")
                print(f"üèõÔ∏è üîç DEBUG: Sample entity dataset: {entities[0].get('dataset', 'NO DATASET FIELD')}")
                print(f"üèõÔ∏è üîç DEBUG: Sample entity reference: {entities[0].get('reference', 'NO REFERENCE')}")
                print(f"üèõÔ∏è üîç DEBUG: Sample entity name: {entities[0].get('name', 'NO NAME')}")
                print(f"üèõÔ∏è üîç DEBUG: First entity full data: {entities[0]}")
            
            planning_summary = {
                'total_applications': len(entities),
                'applications': []
            }
            
            for entity in entities:
                # Extract proper planning application type from reference field
                reference = entity.get('reference', 'N/A')
                planning_type = self._extract_planning_type_from_reference(reference)
                
                # Try multiple field mappings to extract meaningful data
                # Calculate realistic start date (government API has unreliable start-date field)
                start_date_value = entity.get('start-date') 
                
                # Don't use entry-date as it's when data was entered (all show 2025-05-30)
                if not start_date_value or start_date_value.strip() == '':
                    # Calculate approximate start date from decision date (typically 3-6 months before)
                    decision_date = entity.get('decision-date')
                    if decision_date and len(decision_date) >= 10:
                        try:
                            from datetime import datetime, timedelta
                            decision_dt = datetime.strptime(decision_date[:10], '%Y-%m-%d')
                            # Estimate start date as 4 months before decision (realistic timeline)
                            estimated_start = decision_dt - timedelta(days=120)
                            start_date_value = estimated_start.strftime('%Y-%m-%d')
                            print(f"üèõÔ∏è üìÖ Estimated start date {start_date_value} from decision {decision_date}")
                        except Exception as e:
                            # Use decision year but earlier in year if calculation fails
                            try:
                                year = decision_date[:4]
                                start_date_value = f"{year}-01-15"  # January of decision year
                            except:
                                start_date_value = '2023-01-01'
                    else:
                        # Use reference year if available, otherwise default
                        reference = entity.get('reference', '')
                        if '/' in reference and len(reference.split('/')[0]) >= 2:
                            try:
                                ref_year = '20' + reference.split('/')[0]  # Convert '23' to '2023'
                                start_date_value = f"{ref_year}-03-15"  # March of reference year
                            except:
                                start_date_value = '2023-01-01'
                        else:
                            start_date_value = '2023-01-01'
                
                app_data = {
                    'application_type': planning_type,
                    'decision_date': entity.get('decision-date', entity.get('end-date', entity.get('entry-date', 'N/A'))),
                    'name': entity.get('name') or f"Planning Application {reference}",
                    'reference': reference,
                    'description': entity.get('description') or self._extract_description(entity),
                    'applicant': self._extract_applicant_info(entity),
                    'start_date': start_date_value,
                    'organisation': entity.get('organisation-entity', entity.get('organisation', 'UK Government')),
                    'status': entity.get('status', 'Active'),
                    'point': entity.get('point', entity.get('geometry', 'N/A')),
                    'application_url': self._build_planning_application_url(reference, entity.get('organisation-entity'))
                }
                planning_summary['applications'].append(app_data)
            
            return planning_summary
            
        except Exception as e:
            print(f"üèõÔ∏è ‚ö†Ô∏è Error processing government data: {str(e)}")
            return {'error': f'Failed to process government API data: {str(e)}'}
    
    def _extract_planning_type_from_reference(self, reference: str) -> str:
        """Extract planning application type from reference code (e.g., '23/00002/FUL' -> 'Full Planning')"""
        if not reference or reference == 'N/A':
            return 'Planning Application'
        
        # Extract the suffix (application type code) from reference
        parts = reference.split('/')
        if len(parts) >= 3:
            type_code = parts[-1].upper()  # Last part, e.g., 'FUL' from '23/00002/FUL'
        else:
            return 'Planning Application'
        
        # Map planning application type codes to user-friendly names
        type_mapping = {
            'FUL': 'Full Planning Permission',
            'FULM': 'Full Planning Permission (Major)',
            '3FUL': 'Full Planning Permission (Major)',
            'OUT': 'Outline Planning Permission',
            'OUTL': 'Outline Planning Permission',
            'REM': 'Reserved Matters',
            'LBC': 'Listed Building Consent', 
            'ADV': 'Advertisement Consent',
            'HOU': 'Householder Planning Permission',
            'TPO': 'Tree Preservation Order',
            'DEM': 'Demolition',
            'CND': 'Condition Discharge',
            'NMA': 'Non-Material Amendment',
            'S73': 'Section 73 Amendment',
            'COU': 'Change of Use',
            'CAC': 'Conservation Area Consent',
            'CLP': 'Certificate of Lawful Use/Development',
            'TEL': 'Telecommunications',
            'AGR': 'Agricultural',
            'MIN': 'Minerals',
            'WAS': 'Waste',
            'EIA': 'Environmental Impact Assessment'
        }
        
        return type_mapping.get(type_code, f'Planning Application ({type_code})')


    def _extract_applicant_info(self, entity: Dict) -> str:
        """Extract applicant information from government entity - improved version"""
        
        # Debug: Show what applicant fields are actually available
        applicant_debug = {
            'applicant': entity.get('applicant'),
            'name': entity.get('name'), 
            'organisation': entity.get('organisation'),
            'organisation-entity': entity.get('organisation-entity'),
            'description_sample': (entity.get('description', ''))[:100] if entity.get('description') else None
        }
        print(f"üèõÔ∏è üë§ DEBUG APPLICANT FIELDS: {applicant_debug}")
        
        # Try multiple fields to find applicant information
        applicant_fields = [
            entity.get('applicant'),
            entity.get('name'),
            entity.get('organisation'),
            entity.get('applicant_name')
        ]
        
        # Use the first non-empty field
        for field in applicant_fields:
            if field and str(field).strip() and str(field).lower() not in ['', 'n/a', 'none', '0']:
                return str(field).strip()
        
        # If no direct applicant info, try to extract from description
        description = entity.get('description', '')
        if description:
            import re
            
            # Enhanced patterns for UK planning applications
            patterns = [
                r'Applicant[:\s]+([A-Za-z][^,\n.]+?)(?:[,\n.]|$)',
                r'Agent[:\s]+([A-Za-z][^,\n.]+?)(?:[,\n.]|$)', 
                r'On behalf of[:\s]+([A-Za-z][^,\n.]+?)(?:[,\n.]|$)',
                r'For[:\s]+([A-Z][A-Za-z\s&.]{3,40})(?:\s+(?:to|at|for|,)|[,\n.]|$)',
                r'Submitted by[:\s]+([A-Za-z][^,\n.]+?)(?:[,\n.]|$)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, description, re.IGNORECASE)
                if match:
                    applicant = match.group(1).strip()
                    if len(applicant) > 2 and not any(word in applicant.lower() for word in ['the', 'this', 'that', 'erection', 'construction']):
                        return applicant
        
        return 'Applicant not specified'

    def _extract_description(self, entity: Dict) -> str:
        """Extract the best available description from government entity"""
        # Try multiple fields to build a meaningful description
        description_parts = []
        
        if entity.get('description'):
            description_parts.append(entity.get('description'))
        if entity.get('name') and entity.get('name') not in description_parts:
            description_parts.append(f"Name: {entity.get('name')}")
        if entity.get('dataset'):
            description_parts.append(f"Dataset: {entity.get('dataset')}")
        if entity.get('typology'):
            description_parts.append(f"Type: {entity.get('typology')}")
        
        return ' | '.join(description_parts) if description_parts else 'UK Government Entity'
    
    def _get_demo_planning_data(self, application_type: str = None) -> Dict[str, Any]:
        """Return demonstration planning data when APIs are unavailable"""
        demo_apps = [
            {
                'application_type': application_type or 'outline-planning-permission',
                'decision_date': '2024-09-01',
                'name': 'Residential Development Project',
                'reference': 'DEMO/2024/001',
                'description': 'Demonstration planning application data - this would contain real planning application details in a live system',
                'applicant': 'Demo Development Company Ltd',
                'start_date': '2024-08-01',
                'organisation': 'Demo Planning Authority',
                'status': 'Demo Status',
                'point': 'POINT(-1.234 52.678)'
            }
        ]
        
        if application_type and 'outline-planning-permission-‚Äì-some-matters-reserved' in application_type.lower():
            demo_apps.append({
                'application_type': 'outline-planning-permission-‚Äì-some-matters-reserved',
                'decision_date': '2024-09-05',
                'name': 'Mixed Development with Reserved Matters',
                'reference': 'DEMO/2024/002',
                'description': 'Outline planning permission with some matters reserved for mixed-use development including commercial and residential units',
                'applicant': 'Premier Holdings Group',
                'start_date': '2024-08-15',
                'organisation': 'Demo Borough Council',
                'status': 'Pending Decision',
                'point': 'POINT(-1.345 52.789)'
            })
        
        return {
            'total_applications': len(demo_apps),
            'applications': demo_apps
        }
    
    def get_planning_by_postcode(self, postcode: str) -> Dict[str, Any]:
        """Get planning applications by postcode area"""
        try:
            # Note: This is a simplified implementation
            # In practice, you might need to geocode the postcode first
            params = {
                'limit': 20,
                'typology': ['planning-application'],
                'format': 'json'
            }
            
            response = self.session.get(
                f"{self.base_url}/entity.json",
                params=params,
                timeout=10
            )
            
            time.sleep(self.rate_limit_delay)
            
            if response.status_code == 200:
                return self._process_planning_data(response.json())
            else:
                return {'error': f'Postcode search failed: {response.status_code}'}
                
        except Exception as e:
            return {'error': f'Postcode planning search failed: {str(e)}'}
    
    def generate_api_key(self, email: str) -> Dict[str, Any]:
        """Generate a new API key for the given email address"""
        try:
            response = self.session.get(
                f"{self.base_url}/generatekey",
                params={'email': email},
                timeout=10
            )
            
            if response.status_code == 200:
                json_response = response.json()
                return {
                    'success': True,
                    'api_key': json_response.get('response', {}).get('key'),
                    'status': json_response.get('response', {}).get('status')
                }
            else:
                return {
                    'success': False,
                    'error': f'Failed to generate key: {response.status_code}'
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Error generating API key: {str(e)}'
            }
    
    def check_health(self) -> Dict[str, Any]:
        """Check London Planning API health status"""
        health_status = {
            'healthy': False,
            'api_accessible': False,
            'error_message': None
        }
        
        try:
            # Test API access with a simple search
            response = self.session.get(
                self.base_url,
                json={'size': 1, 'query': {'match_all': {}}},
                timeout=10
            )
            
            if response.status_code == 200:
                health_status['api_accessible'] = True
                health_status['healthy'] = True
            else:
                health_status['error_message'] = f"API returned status code: {response.status_code}"
                
        except Exception as e:
            health_status['error_message'] = str(e)
            
        return health_status
    
    def test_api_connection(self) -> bool:
        """Test API connection"""
        if not self.api_key:
            return False
        
        try:
            # Test with a simple search
            response = self.session.get(
                f"{self.base_url}/search",
                params={'key': self.api_key, 'limit': 1},
                timeout=5
            )
            return response.status_code == 200
        except:
            return False

class HunterClient:
    """Client for Hunter.io API for domain search"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.hunter.io"
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json'
        })
    
    def find_company_domain(self, company_name: str) -> Optional[str]:
        """Find domain for a company using Hunter.io API"""
        if not self.api_key or not company_name:
            return None
        
        try:
            # Clean company name
            clean_name = self._clean_company_name(company_name)
            
            # Try v2 API first
            domain = self._call_v2_domain_search(clean_name)
            if domain:
                return domain
            
            # Fallback to v1 API
            domain = self._call_v1_domain_search(clean_name)
            return domain
            
        except Exception as e:
            print(f"Hunter.io domain search failed: {str(e)}")
            return None
    
    def _call_v2_domain_search(self, company_name: str) -> Optional[str]:
        """Call Hunter.io v2 domain search API"""
        try:
            params = {
                'company': company_name,
                'api_key': self.api_key
            }
            
            response = self.session.get(
                f"{self.base_url}/v2/domain-search",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                domain = data.get('data', {}).get('domain')
                if domain and domain.strip():
                    return domain.strip()
            
            return None
            
        except Exception:
            return None
    
    def _call_v1_domain_search(self, company_name: str) -> Optional[str]:
        """Call Hunter.io v1 search API as fallback"""
        try:
            params = {
                'company': company_name,
                'api_key': self.api_key
            }
            
            response = self.session.get(
                f"{self.base_url}/v1/search",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                domain = data.get('data', {}).get('domain')
                if domain and domain.strip():
                    return domain.strip()
            
            return None
            
        except Exception:
            return None
    
    def _clean_company_name(self, company_name: str) -> str:
        """Clean company name for better search results"""
        if not company_name:
            return ""
        
        # Remove common suffixes and clean
        suffixes_to_remove = [
            'LTD', 'LTD.', 'LIMITED', 'LIMITED.', 'PLC', 'PLC.', 
            'CORP', 'CORP.', 'CORPORATION', 'CORPORATION.',
            'INC', 'INC.', 'LLC', 'LLC.', 'LLP', 'LLP.',
            '& COMPANY', '& CO', '& CO.', 'AND COMPANY', 'AND CO'
        ]
        
        # Remove non-breaking spaces and normalize whitespace first
        cleaned = company_name.replace('\u00A0', ' ').strip()
        cleaned = ' '.join(cleaned.split())  # Remove multiple spaces
        cleaned = cleaned.upper()
        
        # Remove suffixes (check for both with and without preceding space)
        for suffix in suffixes_to_remove:
            # Check for suffix at the end with space
            if cleaned.endswith(f' {suffix}'):
                cleaned = cleaned[:-len(f' {suffix}')]
            # Check for suffix at the end without space (for names like "CompanyLTD")
            elif cleaned.endswith(suffix) and len(cleaned) > len(suffix):
                cleaned = cleaned[:-len(suffix)]
        
        # Final cleanup
        cleaned = cleaned.strip()
        
        return cleaned.title() if cleaned else ""
    
    def test_api_connection(self) -> bool:
        """Test API connection using stripe.com as reference"""
        if not self.api_key:
            return False
        
        try:
            params = {
                'domain': 'stripe.com',
                'api_key': self.api_key
            }
            
            response = self.session.get(
                f"{self.base_url}/v2/domain-search",
                params=params,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {}).get('domain') == 'stripe.com'
            
            return False
            
        except Exception:
            return False
    
    def find_emails_by_domain(self, domain: str, first_name: str = None, last_name: str = None, 
                             limit: int = 10) -> List[Dict]:
        """Find email addresses for a specific domain, optionally filtered by name"""
        if not self.api_key or not domain:
            return []
        
        try:
            params = {
                'domain': domain.strip(),
                'api_key': self.api_key,
                'limit': limit
            }
            
            # Add name filters if provided
            if first_name:
                params['first_name'] = first_name.strip()
            if last_name:
                params['last_name'] = last_name.strip()
            
            response = self.session.get(
                f"{self.base_url}/v2/domain-search",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                emails = data.get('data', {}).get('emails', [])
                
                # Format the email results
                formatted_emails = []
                for email_data in emails:
                    formatted_emails.append({
                        'email': email_data.get('value', ''),
                        'first_name': email_data.get('first_name', ''),
                        'last_name': email_data.get('last_name', ''),
                        'position': email_data.get('position', ''),
                        'department': email_data.get('department', ''),
                        'confidence': email_data.get('confidence', 0),
                        'verification': email_data.get('verification', {}).get('result', 'unknown')
                    })
                
                return formatted_emails
            
            return []
            
        except Exception as e:
            print(f"Hunter email search failed: {str(e)}")
            return []
    
    def verify_email(self, email: str) -> Dict[str, Any]:
        """Verify an email address using Hunter.io"""
        if not self.api_key or not email:
            return {'valid': False, 'confidence': 0}
        
        try:
            params = {
                'email': email.strip(),
                'api_key': self.api_key
            }
            
            response = self.session.get(
                f"{self.base_url}/v2/email-verifier",
                params=params,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json().get('data', {})
                return {
                    'email': data.get('email', email),
                    'valid': data.get('result') == 'deliverable',
                    'confidence': data.get('score', 0),
                    'result': data.get('result', 'unknown'),
                    'sources': data.get('sources', [])
                }
            
            return {'valid': False, 'confidence': 0, 'result': 'unknown'}
            
        except Exception as e:
            print(f"Hunter email verification failed: {str(e)}")
            return {'valid': False, 'confidence': 0, 'result': 'error'}
    
    def find_officer_emails(self, officer_name: str, company_domain: str) -> List[Dict]:
        """Find email addresses for a specific officer at a company domain"""
        if not officer_name or not company_domain:
            return []
        
        # Parse the officer name
        name_parts = officer_name.strip().split()
        if len(name_parts) < 2:
            return []
        
        first_name = name_parts[0]
        last_name = name_parts[-1]
        
        # Search for emails with name filters
        emails = self.find_emails_by_domain(company_domain, first_name, last_name)
        
        # Filter results for better matches
        filtered_emails = []
        for email_data in emails:
            # Calculate name match confidence
            email_first = email_data.get('first_name', '').lower()
            email_last = email_data.get('last_name', '').lower()
            
            if (first_name.lower() in email_first or email_first in first_name.lower()) and \
               (last_name.lower() in email_last or email_last in last_name.lower()):
                # Add name matching confidence
                email_data['name_match_confidence'] = 0.9
                filtered_emails.append(email_data)
            elif first_name.lower()[0] == email_first[0:1] and last_name.lower() in email_last:
                # First initial + last name match
                email_data['name_match_confidence'] = 0.7
                filtered_emails.append(email_data)
        
        return filtered_emails
    
    def enrich_company(self, company_data: Dict) -> Optional[Dict]:
        """Enrich company data with domain information"""
        company_name = company_data.get('company_name', '')
        domain = self.find_company_domain(company_name)
        
        if domain:
            return {
                'domain': domain,
                'company_name': company_name
            }
        
        return None


class ApolloClient:
    """Client for Apollo.io API for email discovery and enrichment"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.apollo.io/v1"
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Cache-Control': 'no-cache'
        })
    
    def search_people(self, first_name: str, last_name: str, organization_name: str = None,
                      domain: str = None, limit: int = 10) -> List[Dict]:
        """Search for people using Apollo.io People Search API"""
        if not self.api_key:
            return []
        
        try:
            params = {
                'api_key': self.api_key,
                'first_name': first_name.strip() if first_name else '',
                'last_name': last_name.strip() if last_name else '',
                'per_page': min(limit, 25),  # Apollo.io max per page
                'page': 1
            }
            
            # Add organization filters
            if organization_name:
                params['organization_name'] = organization_name.strip()
            if domain:
                params['organization_domains'] = domain.strip()
            
            response = self.session.get(
                f"{self.base_url}/mixed_people/search",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                people = data.get('people', [])
                
                formatted_results = []
                for person in people:
                    email = person.get('email')
                    if email:  # Only include results with email addresses
                        formatted_results.append({
                            'email': email,
                            'first_name': person.get('first_name', ''),
                            'last_name': person.get('last_name', ''),
                            'title': person.get('title', ''),
                            'organization_name': person.get('organization', {}).get('name', ''),
                            'domain': person.get('organization', {}).get('primary_domain', ''),
                            'linkedin_url': person.get('linkedin_url', ''),
                            'confidence': person.get('email_status', '').lower() == 'verified' and 0.8 or 0.6,
                            'source': 'apollo'
                        })
                
                return formatted_results
            
            return []
            
        except Exception as e:
            print(f"Apollo people search failed: {str(e)}")
            return []
    
    def find_officer_emails(self, officer_name: str, company_name: str, company_domain: str = None) -> List[Dict]:
        """Find email addresses for a specific officer using Apollo.io"""
        if not officer_name:
            return []
        
        # Parse the officer name
        name_parts = officer_name.strip().split()
        if len(name_parts) < 2:
            return []
        
        first_name = name_parts[0]
        last_name = name_parts[-1]
        
        # Search with company name first, then domain if available
        results = []
        
        if company_name:
            results = self.search_people(
                first_name=first_name,
                last_name=last_name,
                organization_name=company_name,
                limit=5
            )
        
        # If no results and we have a domain, try domain search
        if not results and company_domain:
            results = self.search_people(
                first_name=first_name,
                last_name=last_name,
                domain=company_domain,
                limit=5
            )
        
        # Add name matching confidence scores
        for result in results:
            result_first = result.get('first_name', '').lower()
            result_last = result.get('last_name', '').lower()
            
            # Calculate name match confidence
            if (first_name.lower() == result_first and last_name.lower() == result_last):
                result['name_match_confidence'] = 0.95
            elif (first_name.lower() in result_first and last_name.lower() in result_last):
                result['name_match_confidence'] = 0.8
            else:
                result['name_match_confidence'] = 0.6
        
        return results
    
    def verify_email(self, email: str) -> Dict[str, Any]:
        """Verify email address using Apollo.io (if available)"""
        # Apollo.io doesn't have a dedicated email verification endpoint
        # Return a basic response indicating it's from Apollo
        return {
            'email': email,
            'valid': True,  # Assume valid since Apollo provides it
            'confidence': 0.7,  # Medium confidence without dedicated verification
            'result': 'unknown',
            'source': 'apollo'
        }
    
    def enrich_company(self, company_data: Dict) -> Optional[Dict]:
        """Enrich company data using Apollo.io organization search"""
        company_name = company_data.get('company_name', '')
        if not self.api_key or not company_name:
            return None
        
        try:
            params = {
                'api_key': self.api_key,
                'name': company_name.strip(),
                'per_page': 5
            }
            
            response = self.session.get(
                f"{self.base_url}/organizations/search",
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                organizations = data.get('organizations', [])
                
                if organizations:
                    org = organizations[0]  # Take the first (best) match
                    return {
                        'name': org.get('name'),
                        'primary_domain': org.get('primary_domain'),
                        'website_url': org.get('website_url'),
                        'industry': org.get('industry'),
                        'employee_count': org.get('estimated_num_employees'),
                        'founded_year': org.get('founded_year'),
                        'description': org.get('description'),
                        'linkedin_url': org.get('linkedin_url'),
                        'source': 'apollo'
                    }
            
            return None
            
        except Exception as e:
            print(f"Apollo company enrichment failed: {str(e)}")
            return None


class BrightDataClient:
    """Client for Bright Data LinkedIn API"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.brightdata.com/datasets/v3/trigger"
        self.dataset_id = "gd_l1viktl72bvl7bjuj0"  # LinkedIn profiles by name dataset
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        })
    
    def search_linkedin_profile(self, first_name: str, last_name: str, company_name: str) -> Optional[str]:
        """Search for LinkedIn profile using name and company"""
        if not self.api_key:
            return None
        
        try:
            # Correct format for name-based discovery
            params = {
                "dataset_id": self.dataset_id,
                "include_errors": "true",
                "type": "discover_new",
                "discover_by": "name"
            }
            
            # Clean names for better search  
            first_name = self._clean_name(first_name)
            last_name = self._clean_name(last_name)
            
            # Direct JSON array for name-based discovery
            data = [{
                "first_name": first_name,
                "last_name": last_name
            }]
            
            # Debug: Show what we're searching for
            print(f"üîç Searching LinkedIn for: {first_name} {last_name}")
            
            response = self.session.post(self.base_url, json=data, params=params, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                print(f"üìä API Response: {result}")
                
                # Check if we got a snapshot_id (async job started)
                if 'snapshot_id' in result:
                    snapshot_id = result['snapshot_id']
                    print(f"‚è±Ô∏è Job started with snapshot ID: {snapshot_id}")
                    print("‚è≥ Waiting for LinkedIn search results...")
                    
                    # Automatically poll for results
                    results_data = self._fetch_results_by_snapshot(snapshot_id)
                    if results_data:
                        extracted_url = self._extract_linkedin_url(results_data)
                        print(f"üîó Final LinkedIn URL: {extracted_url}")
                        return extracted_url
                    else:
                        return f"Job timeout: {snapshot_id}"
                else:
                    # Direct results (shouldn't happen with /trigger)
                    extracted_url = self._extract_linkedin_url(result)
                    print(f"üîó Extracted URL: {extracted_url}")
                    return extracted_url
            
            return None
            
        except Exception as e:
            print(f"Bright Data LinkedIn search failed: {str(e)}")
            return None
    
    def search_multiple_profiles(self, officers: List[Dict], company_name: str, company_address: str = None) -> Dict[str, str]:
        """Search for multiple LinkedIn profiles at once"""
        if not self.api_key or not officers:
            return {}
        
        try:
            # Prepare batch request
            input_data = []
            for officer in officers[:5]:  # Limit to 5 officers to control costs
                if isinstance(officer, str):
                    # If officer is just a name string, parse it
                    name_parts = officer.strip().split()
                    if len(name_parts) >= 2:
                        first_name = name_parts[0]
                        last_name = name_parts[-1]  # Take only the last name part (no middle names)
                    else:
                        continue
                else:
                    # If officer is a dict with structured data
                    full_name = officer.get('name', '')
                    if not full_name:
                        continue
                    name_parts = full_name.strip().split()
                    if len(name_parts) >= 2:
                        first_name = name_parts[0]
                        last_name = name_parts[-1]  # Take only the last name part (no middle names)
                    else:
                        continue
                
                input_data.append({
                    "first_name": self._clean_name(first_name),
                    "last_name": self._clean_name(last_name),
                    "country_code": "GB"  # Only return UK/Great Britain results
                })
            
            if not input_data:
                return {}
            
            # Correct format for name-based discovery
            params = {
                "dataset_id": self.dataset_id,
                "include_errors": "true",
                "type": "discover_new",
                "discover_by": "name"
            }
            
            # Direct JSON array for batch name-based discovery
            data = input_data
            
            # Debug: Show what names we're searching for
            search_names = [f"{item['first_name']} {item['last_name']}" for item in data]
            st.write(f"üîç Batch searching LinkedIn for: {', '.join(search_names)}")
            
            response = self.session.post(self.base_url, json=data, params=params, timeout=60)
            
            if response.status_code == 200:
                results = response.json()
                st.write(f"üìä Batch API Response: {results}")
                
                # Check if we got a snapshot_id (async job started)
                if 'snapshot_id' in results:
                    snapshot_id = results['snapshot_id']
                    st.write(f"‚è±Ô∏è Batch job started with snapshot ID: {snapshot_id}")
                    st.write("‚è≥ Waiting for LinkedIn search results...")
                    
                    # Automatically poll for results
                    results_data = self._fetch_results_by_snapshot(snapshot_id)
                    if results_data:
                        processed_results = self._process_batch_results(results_data, officers, company_address)
                        st.write(f"üîó Final LinkedIn Results: {processed_results}")
                        return processed_results
                    else:
                        return {"job_timeout": f"Snapshot: {snapshot_id}"}
                else:
                    # Direct results (shouldn't happen with /trigger)
                    processed_results = self._process_batch_results(results, officers, company_address)
                    st.write(f"üîó Processed Results: {processed_results}")
                    return processed_results
            
            return {}
            
        except Exception as e:
            st.warning(f"Bright Data batch LinkedIn search failed: {str(e)}")
            return {}
    
    def _clean_name(self, name: str) -> str:
        """Clean name for better LinkedIn search accuracy"""
        if not name:
            return ""
        
        # Remove common titles and suffixes (less aggressive cleaning for LinkedIn)
        titles_to_remove = [
            'Mr', 'Mrs', 'Ms', 'Miss', 'Dr', 'Prof', 'Sir', 'Dame'
        ]
        
        # Split and clean
        name_parts = name.strip().split()
        cleaned_parts = []
        
        for part in name_parts:
            clean_part = part.strip('.,()[]')
            # Only remove if it's a clear title, keep professional suffixes
            if clean_part not in titles_to_remove and len(clean_part) > 1:
                # Ensure proper capitalization (first letter uppercase, rest lowercase)
                cleaned_parts.append(clean_part.capitalize())
        
        return ' '.join(cleaned_parts)
    
    def _clean_company_name(self, company_name: str) -> str:
        """Clean company name for better search"""
        if not company_name:
            return ""
        
        # Remove common suffixes
        suffixes_to_remove = [
            'LTD', 'LTD.', 'LIMITED', 'LIMITED.', 'PLC', 'PLC.', 
            'CORP', 'CORP.', 'CORPORATION', 'CORPORATION.',
            'INC', 'INC.', 'LLC', 'LLC.', 'LLP', 'LLP.',
            '& COMPANY', '& CO', '& CO.', 'AND COMPANY', 'AND CO'
        ]
        
        # Remove non-breaking spaces and normalize whitespace first
        cleaned = company_name.replace('\u00A0', ' ').strip()
        cleaned = ' '.join(cleaned.split())  # Remove multiple spaces
        cleaned = cleaned.upper()
        
        # Remove suffixes (check for both with and without preceding space)
        for suffix in suffixes_to_remove:
            # Check for suffix at the end with space
            if cleaned.endswith(f' {suffix}'):
                cleaned = cleaned[:-len(f' {suffix}')]
            # Check for suffix at the end without space (for names like "CompanyLTD")
            elif cleaned.endswith(suffix) and len(cleaned) > len(suffix):
                cleaned = cleaned[:-len(suffix)]
        
        # Final cleanup
        cleaned = cleaned.strip()
        
        return cleaned.title() if cleaned else ""
    
    def _fetch_results_by_snapshot(self, snapshot_id: str, max_wait_time: int = 180) -> Optional[Dict]:
        """Fetch results from Bright Data using snapshot ID with automatic polling"""
        if not snapshot_id:
            return None
        
        # Endpoint to fetch snapshot results
        results_url = f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}"
        
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            try:
                response = self.session.get(results_url, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Check if results are ready
                    if isinstance(data, list) and len(data) > 0:
                        st.write(f"‚úÖ Results ready! Found {len(data)} items")
                        return data
                    elif isinstance(data, dict):
                        # Check status
                        if data.get('status') == 'running':
                            st.write("‚è≥ Job still running, waiting...")
                            time.sleep(5)
                            continue
                        elif data.get('status') == 'completed' and 'data' in data:
                            st.write(f"‚úÖ Results ready! Found {len(data.get('data', []))} items")
                            return data.get('data', [])
                        else:
                            st.write(f"üìä Snapshot response: {data}")
                            return data
                
                elif response.status_code == 202:
                    # Job still processing
                    st.write("‚è≥ Job still processing, waiting...")
                    time.sleep(5)
                    continue
                    
                else:
                    st.write(f"‚ùå Error fetching results: HTTP {response.status_code}")
                    break
                    
            except Exception as e:
                st.write(f"‚ùå Error polling results: {str(e)}")
                break
        
        st.write("‚è∞ Timeout waiting for results")
        return None
    
    def _extract_linkedin_url(self, result: Dict, company_city: str = None) -> Optional[str]:
        """Extract LinkedIn URL from API response with GB filtering and city prioritization"""
        try:
            if isinstance(result, list) and len(result) > 0:
                # Filter to only GB results
                gb_profiles = [p for p in result if p.get('country_code') == 'GB']
                
                if not gb_profiles:
                    return None
                
                # If we have company city info, prioritize matching cities
                if company_city and len(gb_profiles) > 1:
                    best_match = self._find_best_city_match(gb_profiles, company_city)
                    if best_match:
                        linkedin_url = best_match.get('url')
                        if linkedin_url and 'linkedin.com' in linkedin_url:
                            return linkedin_url
                
                # Default to first GB profile
                profile = gb_profiles[0]
                linkedin_url = profile.get('url')
                if linkedin_url and 'linkedin.com' in linkedin_url:
                    return linkedin_url
                    
            elif isinstance(result, dict):
                # Single result - check if it's GB
                if result.get('country_code') == 'GB':
                    linkedin_url = result.get('url')
                    if linkedin_url and 'linkedin.com' in linkedin_url:
                        return linkedin_url
            
            return None
        except Exception:
            return None
    
    def _process_batch_results(self, results: List[Dict], original_officers: List, company_address: str = None) -> Dict[str, str]:
        """Process batch results and map to officer names with GB filtering and city prioritization"""
        linkedin_data = {}
        
        try:
            # Extract city from company address if available
            company_city = self._extract_city_from_address(company_address) if company_address else None
            
            for i, result in enumerate(results):
                if i < len(original_officers):
                    officer_name = original_officers[i] if isinstance(original_officers[i], str) else original_officers[i].get('name', '')
                    linkedin_url = self._extract_linkedin_url(result, company_city)
                    if linkedin_url:
                        linkedin_data[officer_name] = linkedin_url
            
            return linkedin_data
            
        except Exception:
            return {}
    
    def _find_best_city_match(self, gb_profiles: List[Dict], company_city: str) -> Optional[Dict]:
        """Find the LinkedIn profile with the best city match"""
        try:
            company_city_lower = company_city.lower().strip()
            
            # Direct city name matches
            for profile in gb_profiles:
                profile_city = profile.get('city', '')
                if profile_city:
                    profile_city_lower = profile_city.lower()
                    # Check for city name in the LinkedIn location
                    if company_city_lower in profile_city_lower:
                        return profile
            
            # Fuzzy matching for common UK city variants
            city_aliases = {
                'london': ['london', 'greater london'],
                'manchester': ['manchester', 'greater manchester'],
                'birmingham': ['birmingham', 'west midlands'],
                'leeds': ['leeds', 'west yorkshire'],
                'glasgow': ['glasgow', 'greater glasgow'],
                'edinburgh': ['edinburgh', 'lothian']
            }
            
            # Check aliases
            for canonical_city, aliases in city_aliases.items():
                if company_city_lower in aliases:
                    for profile in gb_profiles:
                        profile_city = profile.get('city', '').lower()
                        for alias in aliases:
                            if alias in profile_city:
                                return profile
            
            return None
            
        except Exception:
            return None
    
    def _extract_city_from_address(self, address: str) -> Optional[str]:
        """Extract city name from company address"""
        try:
            if not address:
                return None
            
            # Common patterns in UK addresses
            address_lower = address.lower()
            
            # Look for major UK cities
            uk_cities = [
                'london', 'birmingham', 'manchester', 'glasgow', 'edinburgh',
                'leeds', 'sheffield', 'bristol', 'liverpool', 'cardiff',
                'coventry', 'leicester', 'sunderland', 'belfast', 'newcastle',
                'nottingham', 'plymouth', 'wolverhampton', 'stoke', 'derby'
            ]
            
            for city in uk_cities:
                if city in address_lower:
                    return city.title()
            
            # Try to extract from comma-separated address parts
            parts = [part.strip() for part in address.split(',')]
            if len(parts) >= 2:
                # Usually city is the second-to-last or third-to-last component
                potential_city = parts[-2] if len(parts) > 1 else parts[0]
                return potential_city.strip()
            
            return None
            
        except Exception:
            return None

    def enrich_company(self, company_data: Dict) -> Optional[Dict]:
        """Enrich company data - not used for LinkedIn search but kept for compatibility"""
        return None
